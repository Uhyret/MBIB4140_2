{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fcc3fa",
   "metadata": {},
   "source": [
    "### Prerequisites Setup\n",
    "All required dependencies are set up here. This supports a production pipeline approach where reproducibility and environment setup are clearly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2bf85986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I import the necessary libraries\n",
    "# standard\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import unicodedata\n",
    "import urllib\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# third parties\n",
    "import pandas as pd\n",
    "import requests\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from rapidfuzz import process, fuzz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f71a0",
   "metadata": {},
   "source": [
    "## Data Normalization and Metadata Preparation\n",
    "In this section, I normalize event titles using string operations. In this first cell the raw JSON data, located in the folder `Ibsenstage_raw`, is flattened and turned in a Pandas dataframe. In the second cell I remove the `venuecountry` column, since the dataset is focused exclusively on performances in Norway. Later, to standardize the naming of the events, I build a canonical list of titles using the `worktitle` field, then construct regular expressions to match common title variants. Some well-known plays (i.e. `Et dukkehjem`) have additional hardcoded variant patterns (i.e. \"Nora\", \"Casa di bambola\", etc.).\n",
    "The `eventname` field is normalized by matching against the canonical patterns in the third cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bf6ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using source file: ../Ibsenstage_raw/IbsenStage_scrape.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventname</th>\n",
       "      <th>eventid</th>\n",
       "      <th>first_date</th>\n",
       "      <th>workid</th>\n",
       "      <th>worktitle</th>\n",
       "      <th>venueid</th>\n",
       "      <th>venuename</th>\n",
       "      <th>venuecountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85542</td>\n",
       "      <td>1983-11-12</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14985</td>\n",
       "      <td>Honningsvåg kino</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85543</td>\n",
       "      <td>1983-11-14</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14981</td>\n",
       "      <td>Vadsø kino</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85544</td>\n",
       "      <td>1983-11-15</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14983</td>\n",
       "      <td>Miljøbygget</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gengangere</td>\n",
       "      <td>85550</td>\n",
       "      <td>1983-10-14</td>\n",
       "      <td>8542.0</td>\n",
       "      <td>Ghosts</td>\n",
       "      <td>12401</td>\n",
       "      <td>Telemark Teater</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gengangere</td>\n",
       "      <td>85607</td>\n",
       "      <td>1984-01-30</td>\n",
       "      <td>8542.0</td>\n",
       "      <td>Ghosts</td>\n",
       "      <td>12730</td>\n",
       "      <td>Kristiansand Teater</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      eventname  eventid  first_date  workid     worktitle  venueid  \\\n",
       "0  Hedda Gabler    85542  1983-11-12  8547.0  Hedda Gabler    14985   \n",
       "1  Hedda Gabler    85543  1983-11-14  8547.0  Hedda Gabler    14981   \n",
       "2  Hedda Gabler    85544  1983-11-15  8547.0  Hedda Gabler    14983   \n",
       "3    Gengangere    85550  1983-10-14  8542.0        Ghosts    12401   \n",
       "4    Gengangere    85607  1984-01-30  8542.0        Ghosts    12730   \n",
       "\n",
       "             venuename venuecountry  \n",
       "0     Honningsvåg kino       Norway  \n",
       "1           Vadsø kino       Norway  \n",
       "2          Miljøbygget       Norway  \n",
       "3      Telemark Teater       Norway  \n",
       "4  Kristiansand Teater       Norway  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) check whether file exists\n",
    "json_path = '../Ibsenstage_raw/IbsenStage_scrape.json'\n",
    "if not os.path.isfile(json_path):\n",
    "    raise FileNotFoundError(f'File not found: {json_path}')\n",
    "print('Using source file:', json_path)\n",
    "\n",
    "# 2) define output folder path\n",
    "json_out = Path('.') / 'IbsenStage_normalized.json'\n",
    "\n",
    "# 3) flatten and turn into a DataFrame\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    root = json.load(f)\n",
    "records = root.get('hits', root)\n",
    "ibsen_df = pd.json_normalize(records, sep='_')\n",
    "ibsen_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fecec80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"A Doll's House\": [\"^A\\\\ Doll's\\\\ House$\"], 'An Enemy Of The People': ['^An\\\\ Enemy\\\\ Of\\\\ The\\\\ People$'], 'Brand': ['^Brand$'], 'Catiline': ['^Catiline$'], 'Emperor and Galilean': ['^Emperor\\\\ and\\\\ Galilean$'], 'Ghosts': ['^Ghosts$'], 'Hedda Gabler': ['^Hedda\\\\ Gabler$'], 'John Gabriel Borkman': ['^John\\\\ Gabriel\\\\ Borkman$'], 'Lady Inger': ['^Lady\\\\ Inger$'], 'Little Eyolf': ['^Little\\\\ Eyolf$'], \"Love's Comedy\": [\"^Love's\\\\ Comedy$\"], 'Mountain Bird': ['^Mountain\\\\ Bird$'], 'Norma': ['^Norma$'], 'Olaf Liljekrans': ['^Olaf\\\\ Liljekrans$'], 'Peer Gynt': ['^Peer\\\\ Gynt$'], 'Pillars Of Society': ['^Pillars\\\\ Of\\\\ Society$'], 'Poetry': ['^Poetry$'], 'Rosmersholm': ['^Rosmersholm$'], \"St. John's Night\": [\"^St\\\\.\\\\ John's\\\\ Night$\"], 'Svanhild': ['^Svanhild$'], 'Terje Vigen': ['^Terje\\\\ Vigen$'], 'The Burial Mound': ['^The\\\\ Burial\\\\ Mound$'], 'The Feast at Solhaug': ['^The\\\\ Feast\\\\ at\\\\ Solhaug$'], 'The Lady From The Sea': ['^The\\\\ Lady\\\\ From\\\\ The\\\\ Sea$'], 'The League Of Youth': ['^The\\\\ League\\\\ Of\\\\ Youth$'], 'The Master Builder': ['^The\\\\ Master\\\\ Builder$'], 'The Pretenders': ['^The\\\\ Pretenders$'], 'The Vikings at Helgeland': ['^The\\\\ Vikings\\\\ at\\\\ Helgeland$'], 'The Wild Duck': ['^The\\\\ Wild\\\\ Duck$'], 'When We Dead Awaken': ['^When\\\\ We\\\\ Dead\\\\ Awaken$'], 'Et dukkehjem': ['^a doll.*house$', '^ett[\\\\\\\\s_-]*dockhem$', '^casa[\\\\\\\\s_-]*di[\\\\\\\\s_-]*bambola$', '^nora$'], 'Gjengangere': ['^ghosts$', '^spettri$'], 'En folkefiende': ['^an enemy.*people$'], 'Vildanden': ['^the[\\\\\\\\s_-]*wild[\\\\\\\\s_-]*duck$']}\n"
     ]
    }
   ],
   "source": [
    "# 4) remove venuecountry if it exists\n",
    "if 'venuecountry' in ibsen_df.columns:\n",
    "    ibsen_df = ibsen_df.drop(columns=['venuecountry'])\n",
    "\n",
    "# 5) build canonical patterns that are used to normalize titles\n",
    "unique_titles = (\n",
    "    ibsen_df['worktitle']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")\n",
    "canonical = {}\n",
    "for title in unique_titles:\n",
    "    safe = re.escape(title).replace('\\\\\\\\ ', '[\\\\\\\\s_-]*')\n",
    "    canonical[title] = ['^' + safe + '$']\n",
    "\n",
    "extra_variants = {\n",
    "    'Et dukkehjem'   : ['^a doll.*house$', '^ett[\\\\\\\\s_-]*dockhem$', '^casa[\\\\\\\\s_-]*di[\\\\\\\\s_-]*bambola$', '^nora$'],\n",
    "    'Gjengangere'    : ['^ghosts$', '^spettri$'],\n",
    "    'En folkefiende' : ['^an enemy.*people$'],\n",
    "    'Vildanden'      : ['^the[\\\\\\\\s_-]*wild[\\\\\\\\s_-]*duck$'],\n",
    "}\n",
    "for canon, pats in extra_variants.items():\n",
    "    canonical.setdefault(canon, []).extend(pats)\n",
    "\n",
    "pattern_map = [\n",
    "    (re.compile(pat, re.IGNORECASE), canon)\n",
    "    for canon, pats in canonical.items()\n",
    "    for pat in pats\n",
    "]\n",
    "def normalize_title(txt: str) -> str:\n",
    "    if pd.isna(txt): return txt\n",
    "    low = str(txt).strip().lower()\n",
    "    for pat, canon in pattern_map:\n",
    "        if pat.match(low): return canon\n",
    "    return txt\n",
    "\n",
    "print(canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcc7fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventname</th>\n",
       "      <th>eventid</th>\n",
       "      <th>first_date</th>\n",
       "      <th>workid</th>\n",
       "      <th>worktitle</th>\n",
       "      <th>venueid</th>\n",
       "      <th>venuename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85542</td>\n",
       "      <td>1983-11-12</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14985</td>\n",
       "      <td>Honningsvåg kino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85543</td>\n",
       "      <td>1983-11-14</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14981</td>\n",
       "      <td>Vadsø kino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85544</td>\n",
       "      <td>1983-11-15</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14983</td>\n",
       "      <td>Miljøbygget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gengangere</td>\n",
       "      <td>85550</td>\n",
       "      <td>1983-10-14</td>\n",
       "      <td>8542.0</td>\n",
       "      <td>Ghosts</td>\n",
       "      <td>12401</td>\n",
       "      <td>Telemark Teater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gengangere</td>\n",
       "      <td>85607</td>\n",
       "      <td>1984-01-30</td>\n",
       "      <td>8542.0</td>\n",
       "      <td>Ghosts</td>\n",
       "      <td>12730</td>\n",
       "      <td>Kristiansand Teater</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      eventname  eventid  first_date  workid     worktitle  venueid  \\\n",
       "0  Hedda Gabler    85542  1983-11-12  8547.0  Hedda Gabler    14985   \n",
       "1  Hedda Gabler    85543  1983-11-14  8547.0  Hedda Gabler    14981   \n",
       "2  Hedda Gabler    85544  1983-11-15  8547.0  Hedda Gabler    14983   \n",
       "3    Gengangere    85550  1983-10-14  8542.0        Ghosts    12401   \n",
       "4    Gengangere    85607  1984-01-30  8542.0        Ghosts    12730   \n",
       "\n",
       "             venuename  \n",
       "0     Honningsvåg kino  \n",
       "1           Vadsø kino  \n",
       "2          Miljøbygget  \n",
       "3      Telemark Teater  \n",
       "4  Kristiansand Teater  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) normalize eventname using the canonical patterns\n",
    "ibsen_df['eventname'] = ibsen_df['eventname'].apply(normalize_title)\n",
    "\n",
    "# 7) save copy and preview\n",
    "ibsen_df_2 = ibsen_df.copy() \n",
    "print('Preview:')\n",
    "ibsen_df_2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94ef1e",
   "metadata": {},
   "source": [
    "### Preparing the Dataset: working with cities\n",
    "With the following cells I give information about the cities connected to the venues, since knowing the city in which a venue is located will help me in the following steps to enrich data with a city's URI in case a venue's URI will not be available. In the next cells I populate around 60% of the new key `venuecity` with actual city names thanks to GeoPy and GeoNames. \n",
    "To increase the accuracy, if the name of the city or a variation of it is included in the key `venuenames`, it will be mapped in the new key `venuecity` (i.e. \"Teater i Trondheim\" will give \"Trondheim\"). \n",
    "In the first cell I configure a geocoding script with a Nominatim (which finds locations based on the name through OpenStreetMaps), and then cached data is stored in a pickle file (`venue_geocode_cache.pkl`) to avoid stressing the API with redundant calls. There I also try to resolve some incongruencies, by hardcoding some known theares to their respective cities, so that in the case of Oslo the key `venuecity` will connect back to variation of Kristiania or Nationaltheatret. \n",
    "In the second cell I load Norwegian city names from cache/GeoNames API (with hardcoded fallback) and define a function (`get_city_fast`) that extracts city names from venue strings using past overrides, the I implement fuzzy matching (with `process.extractOne`) and text parsing (matching also the genitive form of a cityname).\n",
    "\n",
    "In the next cell I try multiple methods to figure out which Norwegian city each theater venue is located in, by first checking a cache of previous results, then using fast text matching, and finally querying external geocoding APIs as backup options. Lastly I loads the existing cache and processes any new venue names to find their cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b805407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache loaded: 857 entries\n",
      "Loaded 25 theater overrides\n",
      "Normalized override keys: 25\n"
     ]
    }
   ],
   "source": [
    "# configuration of the script\n",
    "DATA_FOLDER = \".\"\n",
    "CACHE_PATH = \"./venue_geocode_cache.pkl\"\n",
    "CACHE_FILE = \"./norway_cities_cache.pkl\"\n",
    "INPUT_FILE = \"./IbsenStage_normalized.json\" \n",
    "OUTPUT_FILE = \"./IbsenStage_with_city.json\" \n",
    "GEONAMES_USER = \"MBIB4140_ibsen_user\"  # your GeoNames username\n",
    "GEONAMES_COUNTRY = \"NO\"\n",
    "\n",
    "# nominatim geocoder setup\n",
    "geolocator = Nominatim(user_agent=\"ibsen_city_extractor\", timeout=5)\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=0.5, max_retries=1)\n",
    "\n",
    "# checks for a cache file and loads it if it exists (FIXED)\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# list of possible location-related keys to check for city names\n",
    "CITY_KEYS = [\n",
    "    \"city\", \"town\", \"village\", \"municipality\", \"hamlet\",\n",
    "    \"locality\", \"county\", \"state_district\", \"state\",\n",
    "    \"region\", \"district\", \"suburb\"\n",
    "]\n",
    "\n",
    "# precompute overrides, maps theater names (or aliases) to known city names\n",
    "OVERRIDES = {\n",
    "    \"nationaltheatret\": \"Oslo\", \"kristiania\": \"Oslo\", \"christiania\": \"Oslo\",\n",
    "    \"det norske teatret\": \"Oslo\", \"black box teater\": \"Oslo\", \"oslo nye\": \"Oslo\",\n",
    "    \"chateau neuf teaterscenen\": \"Oslo\", \n",
    "    \"trøndelag teater\": \"Trondheim\", \"rosendal teater\": \"Trondheim\",\n",
    "    \"den nationale scene\": \"Bergen\", \"dns\": \"Bergen\", \"hordaland teater\": \"Bergen\",\n",
    "    \"kilden\": \"Kristiansand\", \"agder teater\": \"Kristiansand\",\n",
    "    \"hålogaland teater\": \"Tromsø\", \"rogaland teater\": \"Stavanger\",\n",
    "    \"teater innsikt\": \"Stavanger\", \"teater i drammen\": \"Drammen\",\n",
    "    \"telemark teater\": \"Skien\", \"teater Ibsen\": \"Skien\", \"skienhallen\": \"Skien\",\n",
    "    \"teater i moss\": \"Moss\", \"teater i tromsø\": \"Tromsø\", \"Tromsøhallen\": \"Tromsø\",\n",
    "    \"sandnessjøen\": \"Sandnes\"\n",
    "}\n",
    "\n",
    "# standardize text and returns a lowercase, normalized string\n",
    "def normalize(txt: str) -> str:\n",
    "    txt = unicodedata.normalize('NFKD', txt)\n",
    "    txt = \"\".join(c for c in txt if not unicodedata.combining(c))\n",
    "    return re.sub(r'[^a-z0-9]', '', txt.lower())\n",
    "\n",
    "# normalize each override key and create a mapping\n",
    "NORMALIZED_OVERRIDES = {normalize(k): v for k, v in OVERRIDES.items()}\n",
    "NORMALIZED_OVERRIDE_KEYS = list(NORMALIZED_OVERRIDES.keys())\n",
    "\n",
    "print(f\"Cache loaded: {len(cache)} entries\")\n",
    "print(f\"Loaded {len(OVERRIDES)} theater overrides\")\n",
    "print(f\"Normalized override keys: {len(NORMALIZED_OVERRIDE_KEYS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb458dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 69 Norwegian cities\n",
      "Sample cities: ['Sarpsborg', 'Arendal', 'Orkanger', 'Oslo', 'Bergen', 'Vadsø', 'Lillehammer', 'Alta', 'Sandefjord', 'Jessheim']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load cached data about Norway cities\n",
    "def load_norway_cities() -> set:\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    qs = urllib.parse.urlencode({\n",
    "        \"country\": GEONAMES_COUNTRY,\n",
    "        \"featureClass\": \"P\",\n",
    "        \"maxRows\": 2000,\n",
    "        \"username\": GEONAMES_USER\n",
    "    })\n",
    "    url = f\"http://api.geonames.org/searchJSON?{qs}\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=10) as resp:\n",
    "            data = json.load(resp)\n",
    "            cities = {item[\"name\"] for item in data.get(\"geonames\", [])}\n",
    "            # Cache the cities list\n",
    "            with open(CACHE_FILE, \"wb\") as f:\n",
    "                pickle.dump(cities, f)\n",
    "            return cities\n",
    "    except Exception:\n",
    "        # loads a set of Norwegian city names into `norway_cities`, either from a cached file or a hardcoded list, and saves the list to cache\n",
    "        norway_cities = { # this list does not include \"tettsted\" or smaller towns\n",
    "            \"Oslo\", \"Bergen\", \"Trondheim\", \"Stavanger\", \"Kristiansand\", \"Drammen\",\n",
    "            \"Tromsø\", \"Sandnes\", \"Fredrikstad\", \"Sarpsborg\", \"Skien\", \"Ålesund\",\n",
    "            \"Sandefjord\", \"Haugesund\", \"Tønsberg\", \"Moss\", \"Bodø\", \"Arendal\",\n",
    "            \"Hamar\", \"Larvik\", \"Halden\", \"Lillehammer\", \"Mo i Rana\", \"Molde\",\n",
    "            \"Harstad\", \"Kongsberg\", \"Gjøvik\", \"Jessheim\", \"Porsgrunn\", \"Narvik\",\n",
    "            \"Kristiansund\", \"Flekkefjord\", \"Grimstad\", \"Orkanger\", \"Mandal\",\n",
    "            \"Steinkjer\", \"Elverum\", \"Alta\", \"Honefoss\", \"Kongsvinger\",\n",
    "            \"Notodden\", \"Bryne\", \"Otta\", \"Namsos\", \"Fagernes\", \"Røros\",\n",
    "            \"Hammerfest\", \"Kirkenes\", \"Vadsø\", \"Vardø\", \"Sortland\", \"Leknes\",\n",
    "            \"Svolvær\", \"Stokmarknes\", \"Finnsnes\", \"Lenvik\", \"Bardufoss\",\n",
    "            \"Egersund\", \"Levanger\", \"Verdal\", \"Florø\", \"Kolvereid\", \"Voss\",\n",
    "            \"Stjørdal\", \"Kopervik\", \"Mysen\", \"Fauske\", \"Lillesand\", \"Stjørdalshalsen\"\n",
    "        }\n",
    "        with open(CACHE_FILE, \"wb\") as f:\n",
    "            pickle.dump(norway_cities, f)\n",
    "        return norway_cities\n",
    "\n",
    "norway_cities = load_norway_cities()\n",
    "\n",
    "#attempt to extract a Norwegian city name from a venue name string\n",
    "def get_city_fast(venue_name: str) -> str | None:\n",
    "    if not venue_name or pd.isna(venue_name):\n",
    "        return None\n",
    "    \n",
    "    name = venue_name.strip()\n",
    "    key = name.lower()\n",
    "    norm = normalize(key)\n",
    "    \n",
    "    # look for direct matches in overrides\n",
    "    for norm_key, city in NORMALIZED_OVERRIDES.items():\n",
    "        if norm_key in norm:\n",
    "            return city\n",
    "    \n",
    "    \n",
    "    # I try fuzzy matching for close-enough names\n",
    "    if len(norm) > 3:\n",
    "        match, score, *_ = process.extractOne(norm, NORMALIZED_OVERRIDE_KEYS, scorer=fuzz.partial_ratio)\n",
    "        if score > 85: # I chose this threshold to avoid false positives, which is also not too high to miss some matches\n",
    "            return NORMALIZED_OVERRIDES[match]\n",
    "    \n",
    "    # extract city names from venue strings by breaking them into words + check each word against the known city list\n",
    "    for token in re.split(r'[,/()\\-\\s]+', key):\n",
    "        token_cap = token.capitalize()\n",
    "        if token_cap in norway_cities:\n",
    "            return token_cap\n",
    "        \n",
    "        # check for genitive form of the city name (i.e, 'Skiens teater\")\n",
    "        genitive_match = re.match(r'^([A-Za-zÀ-ÿ]+)s$', token_cap)\n",
    "        if genitive_match:\n",
    "            base_city = genitive_match.group(1)\n",
    "            if base_city in norway_cities:\n",
    "                return base_city\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(f\"Loaded {len(norway_cities)} Norwegian cities\")\n",
    "print(f\"Sample cities: {list(norway_cities)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a6206fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocodes a batch of venue names to Norwegian cities using the GeoNames API\n",
    "def batch_geocode_geonames(venue_names, geonames_user) -> dict[str, str | None]:\n",
    "    results = {}\n",
    "    \n",
    "    for venue in venue_names:\n",
    "        if len(venue) <= 2:\n",
    "            results[venue] = None\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            query = urllib.parse.urlencode({\n",
    "                \"q\": venue,\n",
    "                \"country\": \"NO\",\n",
    "                \"maxRows\": 1,\n",
    "                \"username\": geonames_user\n",
    "            })\n",
    "            \n",
    "            url = f\"http://api.geonames.org/searchJSON?{query}\"\n",
    "            with urllib.request.urlopen(url, timeout=3) as resp:\n",
    "                data = json.load(resp)\n",
    "                if data.get(\"geonames\"):\n",
    "                    results[venue] = data[\"geonames\"][0][\"name\"]\n",
    "                else:\n",
    "                    results[venue] = None\n",
    "                    \n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "        except Exception:\n",
    "            results[venue] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# enhanced geocoding using Nominatim with detailed address parsing\n",
    "def batch_geocode_nominatim(venue_names):\n",
    "    results = {venue: None for venue in venue_names}\n",
    "    \n",
    "    for venue in venue_names:\n",
    "        if len(venue) <= 2 or not any(char.isalpha() for char in venue):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            loc = geocode(f\"{venue}, Norway\", addressdetails=True, country_codes=\"no\")\n",
    "            if loc and (addr := loc.raw.get(\"address\")):\n",
    "                # check multiple address fields for city information\n",
    "                for k in CITY_KEYS:\n",
    "                    if k in addr:\n",
    "                        results[venue] = addr[k]\n",
    "                        break\n",
    "                        \n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ensures extraction operations are run only once per venue name, caching results to avoid redundant API calls\n",
    "def get_city_for(venue_name: str) -> str | None:\n",
    "    if not venue_name or pd.isna(venue_name):\n",
    "        return None\n",
    "    \n",
    "    name = venue_name.strip()\n",
    "    \n",
    "    # check cache first\n",
    "    if name in cache:\n",
    "        return cache[name]\n",
    "    \n",
    "    key = name.lower()\n",
    "    norm = normalize(key)\n",
    "\n",
    "    # Try fast matching first\n",
    "    city = get_city_fast(name)\n",
    "    if city:\n",
    "        cache[name] = city\n",
    "        return city\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b98534",
   "metadata": {},
   "source": [
    "In the following cell I prioritize the use of matching city names over Geocode API calls, which despite being more accurate would require many minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0157052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 50/448 venues...\n",
      "  Processed 100/448 venues...\n",
      "  Processed 150/448 venues...\n",
      "  Processed 200/448 venues...\n",
      "  Processed 250/448 venues...\n",
      "  Processed 300/448 venues...\n",
      "  Processed 350/448 venues...\n",
      "  Processed 400/448 venues...\n",
      "venuecity filled for 4924 of 4924 records (100.00%)\n",
      "Total unique venues: 1302\n",
      " Example: 6957 Hyllestad (Hyllestad) → Hyllestad\n",
      "Alvdal\n",
      "None values: 0\n"
     ]
    }
   ],
   "source": [
    "# checks for a cache file and loads it if it exists\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "    # clean existing None values\n",
    "    cache = {k: v for k, v in cache.items() if v is not None}\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# Get unique venue names and filter out already cached ones\n",
    "unique_names = sorted({rec.get(\"venuename\") for rec in records if rec.get(\"venuename\")})\n",
    "uncached_names = [name for name in unique_names if name not in cache]\n",
    "\n",
    "# process venues individually \n",
    "if uncached_names:\n",
    "    start_time = time.time()    \n",
    "    for idx, venue in enumerate(uncached_names, 1):\n",
    "        city = get_city_for(venue)\n",
    "        if idx % 50 == 0:\n",
    "            # save cache periodically\n",
    "            with open(CACHE_PATH, \"wb\") as cf:\n",
    "                pickle.dump(cache, cf)\n",
    "                print(f\"  Processed {idx}/{len(uncached_names)} venues (previously uncached)...\")\n",
    "\n",
    "# annotating records\n",
    "for rec in records:\n",
    "    vn = rec.get(\"venuename\") or \"not found\"\n",
    "    rec[\"venuecity\"] = get_city_for(vn) or \"not found\"\n",
    "    # Stats\n",
    "filled = sum(1 for r in records if r.get(\"venuecity\") and r[\"venuecity\"].strip())\n",
    "total_records = len(records)\n",
    "print(f\"venuecity filled for {filled} of {total_records} records ({(filled / total_records) * 100:.2f}%)\")\n",
    "\n",
    "# save results\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(CACHE_PATH, \"wb\") as cf:\n",
    "    pickle.dump(cache, cf)\n",
    "    \n",
    "print(f\"Total unique venues: {len(unique_names)}\")\n",
    "print(f\" Example: {list(unique_names)[0]} → {cache.get(list(unique_names)[0], 'Not found')}\")\n",
    "print(cache.get(list(unique_names)[15], 'Not found'))\n",
    "print(f\"None values: {list(cache.values()).count(None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bbd3a",
   "metadata": {},
   "source": [
    "The script above pre-processes only those unique venue names for geocoding, and later all 4900+ records are annotated using the chached lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960eaf20",
   "metadata": {},
   "source": [
    "### Mapping Works and Venues to authoritative IDs\n",
    "\n",
    "The next step is to map every key `workid` and `venueid` with an authoritative URI, to resolve external Wikidata URIs for works/venue. To achieve this result, the optimal way is to keep the original keys for `workid` and `venueid` from IbsenStage and add the keys `workURI` and `venueURI` to the json. First I will map the works and then the venues.\n",
    "\n",
    "In the first cell, a SPARQL query is sent to the Wikidata endpoint to retrieve all known works (`wdt:P800`) attributed to Henrik Ibsen (`wd:Q36661`). Labels are filtered to include multiple languages (en, no, nb, nn) and normalized to lowercase for matching, this continues in the second cell where I use the Wikidata results to create a dictionary mapping Ibsen play titles (lowercase) to their Wikidata QIDs. In the third cell then I attempt to match each worktitle from the dataset to a Wikidata URI in two passes:\n",
    "\n",
    "1. Exact match based on normalized title.\n",
    "\n",
    "2. Partial string match (i.e. “Gjengangere” may match “Ghosts” or “The Ghosts”).\n",
    "\n",
    "If a match is found, a new field workURI is added with the corresponding Wikidata QID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ce556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Ibsen works from Wikidata...\n",
      "Found 57 labels from Wikidata (including hardcoded St. John's Eve)\n",
      "First 5 results from Wikidata:\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q176465'}, 'label': {'xml:lang': 'en', 'type': 'literal', 'value': 'Hedda Gabler'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q176465'}, 'label': {'xml:lang': 'nb', 'type': 'literal', 'value': 'Hedda Gabler'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q176465'}, 'label': {'xml:lang': 'nn', 'type': 'literal', 'value': 'Hedda Gabler'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q208094'}, 'label': {'xml:lang': 'en', 'type': 'literal', 'value': 'Peer Gynt'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q208094'}, 'label': {'xml:lang': 'nb', 'type': 'literal', 'value': 'Peer Gynt'}}\n"
     ]
    }
   ],
   "source": [
    "# read the file containing theater data with city information\n",
    "with open('IbsenStage_with_city.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# convert to pandas DataFrame for easier data manipulation\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# query Wikidata for Henrik Ibsen's notable works, direct SPARQL query\n",
    "def get_ibsen_works() -> dict:\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT ?work ?label WHERE {\n",
    "      wd:Q36661 wdt:P800 ?work .\n",
    "      ?work rdfs:label ?label .\n",
    "      FILTER(LANG(?label) IN (\"en\", \"no\", \"nb\", \"nn\"))\n",
    "    }\n",
    "    \"\"\"\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {'User-Agent': 'IbsenStage-Pipeline/1.0'}  # Polite API usage\n",
    "    params = {'query': sparql_query, 'format': 'json'}\n",
    "    \n",
    "    # make the API request to Wikidata\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # return parsed JSON response\n",
    "    else:\n",
    "        raise Exception(f\"SPARQL query failed with status code {response.status_code}\")\n",
    "\n",
    "print(\"Fetching Ibsen works from Wikidata...\")\n",
    "# execute the query and get results\n",
    "wikidata_result = get_ibsen_works()\n",
    "\n",
    "# build title → QID mapping from SPARQL result\n",
    "wikidata_mapping = {}\n",
    "for item in wikidata_result['results']['bindings']:\n",
    "    uri = item['work']['value']\n",
    "    qid = uri.split('/')[-1]  # Extract QID from URI\n",
    "    label = item['label']['value'].lower().strip()\n",
    "    wikidata_mapping[label] = qid\n",
    "\n",
    "# hardcode \"St. John's Eve\" mapping since it is the *only* title not caught by the query\n",
    "wikidata_mapping[\"st. john's night\"] = \"Q3285337\"\n",
    "wikidata_mapping[\"sankthansnatten\"] = \"Q3285337\"\n",
    "wikidata_mapping[\"sanchthansnatten\"] = \"Q3285337\"\n",
    "\n",
    "print(f\"Found {len(wikidata_mapping)} labels from Wikidata (including hardcoded St. John's Eve)\")\n",
    "\n",
    "# preview first 5 entries\n",
    "print(\"First 5 results from Wikidata:\")\n",
    "for item in wikidata_result['results']['bindings'][:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd5c8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 labels from Wikidata\n",
      "First 5 label → QID mappings:\n",
      "hedda gabler → Q176465\n",
      "peer gynt → Q208094\n",
      "emperor and galilean → Q268276\n",
      "kejser og galilæer → Q268276\n",
      "john gabriel borkman → Q289117\n"
     ]
    }
   ],
   "source": [
    "# build title → QID mapping from SPARQL result\n",
    "wikidata_mapping = {}\n",
    "#process each work returned from the Wikidata query\n",
    "for item in wikidata_result['results']['bindings']:\n",
    "    uri = item['work']['value']\n",
    "    qid = uri.split('/')[-1]  # extract QID from URI\n",
    "    label = item['label']['value'].lower().strip()\n",
    "    wikidata_mapping[label] = qid\n",
    "\n",
    "print(f\"Found {len(wikidata_mapping)} labels from Wikidata\")\n",
    "\n",
    "# show first 5 items in the mapping\n",
    "print(\"First 5 label → QID mappings:\")\n",
    "for label, qid in list(wikidata_mapping.items())[:5]:\n",
    "    print(f\"{label} → {qid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dbf4f3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping 'worktitle' to Wikidata QIDs...\n",
      "Saved updated file to: IbsenStage_with_wikidata_works.json\n",
      "Successfully mapped 4905 out of 4924 work titles (99.6%)\n"
     ]
    }
   ],
   "source": [
    "# Maps a work title to its Wikidata QID using exact and fuzzy matching, return QID or None\n",
    "def map_to_qid(title) -> str | None:\n",
    "   # handle empty or null titles\n",
    "   if pd.isna(title) or not title.strip():\n",
    "       return None\n",
    "   title_norm = title.lower().strip() \n",
    "   \n",
    "   # check if I have an exact match in our mapping\n",
    "   if title_norm in wikidata_mapping:\n",
    "       return wikidata_mapping[title_norm]\n",
    "   \n",
    "   # look for substring matches\n",
    "   for label, qid in wikidata_mapping.items():\n",
    "       # check if title contains label or label contains title\n",
    "       if title_norm in label or label in title_norm:\n",
    "           return qid\n",
    "   \n",
    "   return None  # No match found\n",
    "\n",
    "# apply mapping to dataset\n",
    "print(\"Mapping 'worktitle' to Wikidata QIDs...\")\n",
    "df['workURI'] = df['worktitle'].apply(map_to_qid)\n",
    "\n",
    "# save to file\n",
    "output_path = 'IbsenStage_with_wikidata_works.json'\n",
    "# convert DataFrame back to list of records and save as JSON\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "   json.dump(df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved updated file to: {output_path}\")\n",
    "\n",
    "#count how many titles I successfully mapped to QIDs\n",
    "mapped_count = df['workURI'].notna().sum()\n",
    "total_count = len(df)\n",
    "print(f\"Successfully mapped {mapped_count} out of {total_count} work titles ({mapped_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878ffcc",
   "metadata": {},
   "source": [
    "Now I can proceed by connecting `venueURI` to authoritative identifiers from Wikidata, where available.\n",
    "\n",
    "As expected, retrieving and mapping Wikidata URIs for all venues is a challenging process. Many venues are too small or obscure to have their own Wikidata item, or they may be part of a larger building. Some may no longer exist, as the IbsenStage dataset includes historical venues dating back to the 19th century.\n",
    "To address this, I use a two-step fallback strategy:\n",
    "\n",
    "1. Attempt to resolve the venuename to a `venueURI` by querying the Wikidata Search API (wbsearchentities).\n",
    "\n",
    "2. If no match is found, attempt to resolve the `venuecity` instead and store the resulting URI under the key `cityURI`.\n",
    "\n",
    "This method ensures that even if a venue does not have its own Wikidata item, the city associated with it can serve as a proxy reference.\n",
    "To improve performance, I use `aiohttp` for asynchronous HTTP requests and `asyncio.gather()` to run all tasks concurrently (with a semaphore to limit concurrency to `MAX_WORKERS`). Despite this optimization, the cell may still take several minutes to complete (around 7) due to the delay between requests and the need to wait for responses from the Wikidata servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bef02610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4924 records from 'IbsenStage_with_wikidata_works.json' → 'IbsenStage_with_uris.json'\n",
      "Using 5 workers with 0.1s delays\n",
      "User agent: VenueCityWikidataLinker/1.0\n",
      "\n",
      "First few records:\n",
      "  1. Honningsvåg kino (not found)\n",
      "  2. Vadsø kino (Vadsø)\n",
      "  3. Miljøbygget (Trondheim)\n"
     ]
    }
   ],
   "source": [
    "INPUT = \"IbsenStage_with_wikidata_works.json\"\n",
    "OUTPUT = \"IbsenStage_with_uris.json\"\n",
    "USER_AGENT = \"VenueCityWikidataLinker/1.0\"\n",
    "MAX_WORKERS = 5\n",
    "DELAY = 0.1\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# file path\n",
    "def stage_path(filename):\n",
    "    return Path.cwd() / filename\n",
    "\n",
    "# loading data\n",
    "with open(INPUT, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# printing samples\n",
    "print(f\"Processing {len(data)} records from '{INPUT}' → '{OUTPUT}'\")\n",
    "print(f\"Using {MAX_WORKERS} workers with {DELAY}s delays\")\n",
    "print(f\"User agent: {USER_AGENT}\")\n",
    "\n",
    "print(\"\\nFirst few records:\")\n",
    "for i, record in enumerate(data[:3]):\n",
    "   venue = record.get(\"venuename\", \"N/A\")\n",
    "   city = record.get(\"venuecity\", \"N/A\")\n",
    "   print(f\"  {i+1}. {venue} ({city})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc8a618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search Wikidata for an entity and return its QID if found\n",
    "async def query_wikidata(session, search_term: str, semaphore) -> str | None:\n",
    "   if not search_term:\n",
    "       return None\n",
    "   \n",
    "   async with semaphore:  # rate limiting\n",
    "       await asyncio.sleep(DELAY)\n",
    "       # building the Wikidata search API request\n",
    "       params = urllib.parse.urlencode({\n",
    "           \"action\": \"wbsearchentities\",\n",
    "           \"format\": \"json\",\n",
    "           \"language\": \"en\",\n",
    "           \"search\": search_term,\n",
    "           \"limit\": 1,  # I just want the best match\n",
    "           \"type\": \"item\"\n",
    "       })\n",
    "       url = f\"https://www.wikidata.org/w/api.php?{params}\"\n",
    "       headers = {\"User-Agent\": USER_AGENT}\n",
    "       \n",
    "       try:\n",
    "           async with session.get(url, headers=headers, timeout=10) as response:\n",
    "               result = await response.json()\n",
    "               # return the QID of the best match\n",
    "               if result.get(\"search\"):\n",
    "                   return result[\"search\"][0][\"id\"]\n",
    "       except Exception as e:\n",
    "           logger.debug(f\"Wikidata lookup failed for '{search_term}': {e}\")\n",
    "       return None\n",
    "\n",
    "# try to find Wikidata URIs for venue first, then city as fallback.\n",
    "async def resolve_entry(session, entry: dict, semaphore) -> dict:\n",
    "   venue = (entry.get(\"venuename\") or \"\").strip()\n",
    "   city = (entry.get(\"venuecity\") or \"\").strip()\n",
    "   \n",
    "   # venue lookup\n",
    "   venue_uri = await query_wikidata(session, venue, semaphore)\n",
    "   if venue_uri:\n",
    "       entry[\"venueURI\"] = venue_uri\n",
    "   else:\n",
    "       # fallback to city if venue not found\n",
    "       city_uri = await query_wikidata(session, city, semaphore)\n",
    "       if city_uri:\n",
    "           entry[\"cityURI\"] = city_uri\n",
    "   \n",
    "   return entry\n",
    "\n",
    "# process all entries concurrently with rate limiting\n",
    "async def resolve_all(entries: list[dict]) -> list[dict]:\n",
    "   semaphore = asyncio.Semaphore(MAX_WORKERS)  # Control concurrency\n",
    "   async with aiohttp.ClientSession() as session:\n",
    "       # Create tasks for all entries and run them concurrently\n",
    "       tasks = [resolve_entry(session, entry, semaphore) for entry in entries]\n",
    "       return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "49871a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Wikidata URI resolution...\n",
      "Completed in 439.22 seconds\n",
      "\n",
      "Found 4923 URIs from 4924 records (100.0%)\n",
      "  • 2883 venues, 2040 cities\n",
      "\n",
      "Sample resolved entries:\n",
      "  1. Honningsvåg kino (not found) → city:Q404\n",
      "  2. Vadsø kino (Vadsø) → city:Q104379\n",
      "  3. Miljøbygget (Trondheim) → city:Q25804\n",
      "\n",
      "Results saved to IbsenStage_with_uris.json\n"
     ]
    }
   ],
   "source": [
    "# resolution with await\n",
    "print(\"\\nStarting Wikidata URI resolution...\")\n",
    "start_time = time.time()\n",
    "results = await resolve_all(data)\n",
    "processing_time = time.time() - start_time\n",
    "print(f\"Completed in {processing_time:.2f} seconds\")\n",
    "\n",
    "# stats\n",
    "venue_uris = sum(1 for r in results if r.get(\"venueURI\"))\n",
    "city_uris = sum(1 for r in results if r.get(\"cityURI\"))\n",
    "total_uris = venue_uris + city_uris\n",
    "\n",
    "print(f\"\\nFound {total_uris} URIs from {len(results)} records ({(total_uris/len(results)*100):.1f}%)\")\n",
    "print(f\"  • {venue_uris} venues, {city_uris} cities\")\n",
    "\n",
    "print(\"\\nSample resolved entries:\")\n",
    "uri_samples = [r for r in results if r.get(\"venueURI\") or r.get(\"cityURI\")][:3]\n",
    "for i, record in enumerate(uri_samples):\n",
    "   venue = record.get(\"venuename\", \"N/A\")\n",
    "   city = record.get(\"venuecity\", \"N/A\")\n",
    "   venue_uri = record.get(\"venueURI\", \"\")\n",
    "   city_uri = record.get(\"cityURI\", \"\")\n",
    "   \n",
    "   # Show which URI we found\n",
    "   uri_info = f\"venue:{venue_uri}\" if venue_uri else f\"city:{city_uri}\"\n",
    "   print(f\"  {i+1}. {venue} ({city}) → {uri_info}\")\n",
    "\n",
    "# saving output\n",
    "with open(stage_path(OUTPUT), \"w\", encoding=\"utf-8\") as f:\n",
    "   json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d6753",
   "metadata": {},
   "source": [
    "With the fallback strategy, around 58% of venues and respective URI are found and mapped, and other additional 41% city URI complete the mapping, covering almost all the cases.Now that both work and venue IDs and URIs are present, we can bridge them in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e457672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enriched data with URIs...\n",
      "Built bridges: 28 works, 780 venues\n",
      "Bridge saved to ./id_to_uri_bridge.json\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "print(\"Loading enriched data with URIs...\")\n",
    "with open('../Ibsenstage_staged/IbsenStage_with_uris.json', encoding='utf-8') as f:\n",
    "   data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# extract unique work ID → URI mappings\n",
    "work_bridge = (\n",
    "   df[['workid', 'workURI']]\n",
    "   .dropna()  # Skip entries without URIs\n",
    "   .drop_duplicates() \n",
    "   .set_index('workid')['workURI']\n",
    "   .to_dict()\n",
    ")\n",
    "\n",
    "# extract unique venue ID → URI mappings\n",
    "venue_bridge = (\n",
    "   df[['venueid', 'venueURI']]\n",
    "   .dropna()\n",
    "   .drop_duplicates()\n",
    "   .set_index('venueid')['venueURI']\n",
    "   .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Built bridges: {len(work_bridge)} works, {len(venue_bridge)} venues\")\n",
    "\n",
    "# combine into one object\n",
    "bridge = {\n",
    "   'work_bridge': work_bridge,\n",
    "   'venue_bridge': venue_bridge\n",
    "}\n",
    "\n",
    "# save the mapping\n",
    "output_path = \"./id_to_uri_bridge.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "   json.dump(bridge, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Bridge saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45218e83",
   "metadata": {},
   "source": [
    "The next step is converting the JSON `IbsenStage_with_uris` to RDF, this will continue in the file `code2_IbsenStage_curated`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48957f8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
