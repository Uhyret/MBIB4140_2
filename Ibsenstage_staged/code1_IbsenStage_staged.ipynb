{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15fcc3fa",
   "metadata": {},
   "source": [
    "### Prerequisites Setup\n",
    "All required dependencies are set up here. This supports a production pipeline approach where reproducibility and environment setup are clearly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf85986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: geopy==2.4.1 in c:\\users\\cristiano (cc)\\appdata\\roaming\\python\\python312\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\cristiano (cc)\\appdata\\roaming\\python\\python312\\site-packages (from geopy==2.4.1) (2.0)\n"
     ]
    }
   ],
   "source": [
    "# First I import the necessary libraries\n",
    "# standard\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.error import HTTPError, URLError\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "# third parties\n",
    "import pandas as pd\n",
    "import requests\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Here I also install Geopy, which is used for geocoding.\n",
    "!{sys.executable} -m pip install geopy==2.4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f71a0",
   "metadata": {},
   "source": [
    "## Data Normalization and Metadata Preparation\n",
    "In this section, I normalize event titles using string operations. The raw JSON data, located in the folder `Ibsenstage_raw`, is flattened using `pandas.json_normalize()` for easier manipulation.\n",
    "I remove the `venuecountry` column, since the dataset is focused exclusively on performances in Norway. To standardize the naming of events, I build a canonical list of titles using the `worktitle` field, then construct regular expressions to match common title variants. Some well-known plays (i.e. `Et dukkehjem`) have additional hardcoded variant patterns (i.e. \"Nora\", \"Casa di bambola\", etc.).\n",
    "The `eventname` field is then normalized by matching against these compiled regex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf6ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using source file: ../Ibsenstage_raw/IbsenStage_scrape.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventname</th>\n",
       "      <th>eventid</th>\n",
       "      <th>first_date</th>\n",
       "      <th>workid</th>\n",
       "      <th>worktitle</th>\n",
       "      <th>venueid</th>\n",
       "      <th>venuename</th>\n",
       "      <th>venuecountry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85542</td>\n",
       "      <td>1983-11-12</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14985</td>\n",
       "      <td>Honningsvåg kino</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85543</td>\n",
       "      <td>1983-11-14</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14981</td>\n",
       "      <td>Vadsø kino</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>85544</td>\n",
       "      <td>1983-11-15</td>\n",
       "      <td>8547.0</td>\n",
       "      <td>Hedda Gabler</td>\n",
       "      <td>14983</td>\n",
       "      <td>Miljøbygget</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gengangere</td>\n",
       "      <td>85550</td>\n",
       "      <td>1983-10-14</td>\n",
       "      <td>8542.0</td>\n",
       "      <td>Ghosts</td>\n",
       "      <td>12401</td>\n",
       "      <td>Telemark Teater</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gengangere</td>\n",
       "      <td>85607</td>\n",
       "      <td>1984-01-30</td>\n",
       "      <td>8542.0</td>\n",
       "      <td>Ghosts</td>\n",
       "      <td>12730</td>\n",
       "      <td>Kristiansand Teater</td>\n",
       "      <td>Norway</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      eventname  eventid  first_date  workid     worktitle  venueid  \\\n",
       "0  Hedda Gabler    85542  1983-11-12  8547.0  Hedda Gabler    14985   \n",
       "1  Hedda Gabler    85543  1983-11-14  8547.0  Hedda Gabler    14981   \n",
       "2  Hedda Gabler    85544  1983-11-15  8547.0  Hedda Gabler    14983   \n",
       "3    Gengangere    85550  1983-10-14  8542.0        Ghosts    12401   \n",
       "4    Gengangere    85607  1984-01-30  8542.0        Ghosts    12730   \n",
       "\n",
       "             venuename venuecountry  \n",
       "0     Honningsvåg kino       Norway  \n",
       "1           Vadsø kino       Norway  \n",
       "2          Miljøbygget       Norway  \n",
       "3      Telemark Teater       Norway  \n",
       "4  Kristiansand Teater       Norway  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) I open the json file\n",
    "json_path = '../Ibsenstage_raw/IbsenStage_scrape.json'\n",
    "if not os.path.isfile(json_path):\n",
    "    raise FileNotFoundError(f'File not found: {json_path}')\n",
    "print('Using source file:', json_path)\n",
    "\n",
    "# 2) Ensure output folder exists\n",
    "staged_dir = Path('.')\n",
    "staged_dir.mkdir(exist_ok=True)\n",
    "json_out = staged_dir / 'IbsenStage_normalized.json'\n",
    "\n",
    "# 3) Load & flatten JSON\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    root = json.load(f)\n",
    "records = root.get('hits', root)\n",
    "ibsen_df = pd.json_normalize(records, sep='_')\n",
    "ibsen_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecec80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved normalized JSON → IbsenStage_normalized.json\n",
      "Preview:\n",
      "[\n",
      "  {\n",
      "    \"eventname\": \"Hedda Gabler\",\n",
      "    \"eventid\": 85542,\n",
      "    \"first_date\": \"1983-11-12\",\n",
      "    \"workid\": 8547.0,\n",
      "    \"worktitle\": \"Hedda Gabler\",\n",
      "    \"venueid\": 14985,\n",
      "    \"venuename\": \"Honningsvåg kino\"\n",
      "  },\n",
      "  {\n",
      "    \"eventname\": \"Hedda Gabler\",\n",
      "    \"eventid\": 85543,\n",
      "    \"first_date\": \"1983-11-14\",\n",
      "    \"workid\": 8547.0,\n",
      "    \"worktitle\": \"Hedda Gabler\",\n",
      "    \"venueid\": 14981,\n",
      "    \"venuename\": \"Vadsø kino\"\n",
      "  },\n",
      "  {\n",
      "    \"eventname\": \"Hedda Gabler\",\n",
      "    \"eventid\": 85544,\n",
      "    \"first_date\": \"1983-11-15\",\n",
      "    \"workid\": 8547.0,\n",
      "    \"worktitle\": \"Hedda Gabler\",\n",
      "    \"venueid\": 14983,\n",
      "    \"venuename\": \"Miljøbygget\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 4) Remove unnecessary columns\n",
    "if 'venuecountry' in ibsen_df.columns:\n",
    "    ibsen_df = ibsen_df.drop(columns=['venuecountry'])\n",
    "\n",
    "# 5) Build canonical patterns for normalization\n",
    "unique_titles = (\n",
    "    ibsen_df['worktitle']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")\n",
    "canonical = {}\n",
    "for title in unique_titles:\n",
    "    safe = re.escape(title).replace('\\\\\\\\ ', '[\\\\\\\\s_-]*')\n",
    "    canonical[title] = ['^' + safe + '$']\n",
    "\n",
    "extra_variants = {\n",
    "    'Et dukkehjem'   : ['^a doll.*house$', '^ett[\\\\\\\\s_-]*dockhem$', '^casa[\\\\\\\\s_-]*di[\\\\\\\\s_-]*bambola$', '^nora$'],\n",
    "    'Gjengangere'    : ['^ghosts$', '^spettri$'],\n",
    "    'En folkefiende' : ['^an enemy.*people$'],\n",
    "    'Vildanden'      : ['^the[\\\\\\\\s_-]*wild[\\\\\\\\s_-]*duck$'],\n",
    "}\n",
    "for canon, pats in extra_variants.items():\n",
    "    canonical.setdefault(canon, []).extend(pats)\n",
    "\n",
    "pattern_map = [\n",
    "    (re.compile(pat, re.IGNORECASE), canon)\n",
    "    for canon, pats in canonical.items()\n",
    "    for pat in pats\n",
    "]\n",
    "def normalize_title(txt):\n",
    "    if pd.isna(txt): return txt\n",
    "    low = str(txt).strip().lower()\n",
    "    for pat, canon in pattern_map:\n",
    "        if pat.match(low): return canon\n",
    "    return txt\n",
    "\n",
    "# 6) Normalize eventname\n",
    "ibsen_df['eventname'] = ibsen_df['eventname'].apply(normalize_title)\n",
    "\n",
    "# 7) Save the normalized DataFrame to JSON\n",
    "json_out = 'IbsenStage_normalized.json'\n",
    "ibsen_df.to_json(json_out, orient='records', force_ascii=False, indent=2)\n",
    "print('Saved normalized JSON →', json_out)\n",
    "\n",
    "# 8) Preview\n",
    "preview = json.loads(ibsen_df.head(3).to_json(orient='records', force_ascii=False))\n",
    "print('Preview:')\n",
    "print(json.dumps(preview, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94ef1e",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "The dataset is loaded and prepared with necessary libraries. This step ensures that I can apply transformations in a controlled and repeatable environment. I follow FAIR principles, especially focusing on reusability and interoperability. With this cell I give information about the cities connected to the venues, populating over 60% of the keys venuecity with actual city names thanks to GeoPy and GeoNames. \n",
    "To increase the accuracy, if the name of the city or a variation of it is included in the key `venuenames`, it will be mapped in the new key `venuecity` (i.e. \"Teater i Trondheim\" will give \"Trondheim\"). \n",
    "\n",
    "Here I also try to resolve some incongruencies, expecially related to Oslo, so that the key `venuecity` will connect back to that city in the instances of variation of Kristiania or Nationaltheatret. I also include some more common overrides to enhance the population. Ensuring the presence of `venuecity` is meaningful for a fallback when I will map `venueid` to URIs.\n",
    "\n",
    "I initialize a persistent cache to store previously resolved venue-to-city mappings (`venue_geocode_cache.pkl`). A separate city list from the GeoNames API (limited to Norway) is also cached to avoid repeated calls. Unicode normalization is applied to venue names to reduce inconsistencies due to accents or formatting, and the resolved cities are added to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b805407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION AND SETUP COMPLETE\n",
      "Data folder: . = IbsenStage_staged\n",
      "Cache path: .\\venue_geocode_cache.pkl\n",
      "Input file: .\\IbsenStage_normalized.json\n",
      "Output file: .\\IbsenStage_with_city.json\n",
      "GeoNames user: MBIB4140_ibsen_user\n",
      "Cache loaded: 1302 entries\n",
      "Sample Norway cities: None\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_FOLDER     = \".\"\n",
    "CACHE_PATH      = os.path.join(DATA_FOLDER, \"venue_geocode_cache.pkl\")\n",
    "INPUT_FILE      = os.path.join(DATA_FOLDER, \"IbsenStage_normalized.json\")\n",
    "OUTPUT_FILE     = os.path.join(DATA_FOLDER, \"IbsenStage_with_city.json\")\n",
    "GEONAMES_USER   = \"MBIB4140_ibsen_user\"  # your GeoNames username\n",
    "GEONAMES_COUNTRY = \"NO\"\n",
    "\n",
    "# Nominatim Setup\n",
    "geolocator = Nominatim(user_agent=\"ibsen_city_extractor\", timeout=5)  # Reduced timeout\n",
    "geocode    = RateLimiter(geolocator.geocode, min_delay_seconds=0.5, max_retries=1)  # Faster rate limiting\n",
    "\n",
    "# Cache\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# Norway cities list (cached)\n",
    "def load_norway_cities():\n",
    "    cache_file = os.path.join(DATA_FOLDER, \"norway_cities_cache.pkl\")\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    qs = urllib.parse.urlencode({\n",
    "        \"country\": GEONAMES_COUNTRY,\n",
    "        \"featureClass\": \"P\",\n",
    "        \"maxRows\": 2000,\n",
    "        \"username\": GEONAMES_USER\n",
    "    })\n",
    "    url = f\"http://api.geonames.org/searchJSON?{qs}\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=10) as resp:\n",
    "            data = json.load(resp)\n",
    "            cities = {item[\"name\"] for item in data.get(\"geonames\", [])}\n",
    "            # Cache the cities list\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(cities, f)\n",
    "            return cities\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "norway_cities = load_norway_cities()\n",
    "\n",
    "# Print intermediate results\n",
    "print(\"CONFIGURATION AND SETUP COMPLETE\")\n",
    "print(f\"Data folder: {DATA_FOLDER} = IbsenStage_staged\")\n",
    "print(f\"Cache path: {CACHE_PATH}\")\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(f\"GeoNames user: {GEONAMES_USER}\")\n",
    "print(f\"Cache loaded: {len(cache)} entries\")\n",
    "print(\"Sample Norway cities:\", list(norway_cities)[:10] if norway_cities else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb458dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Total unique venues: 1302\n",
      "Already cached: 1297\n",
      "Need to process: 5\n"
     ]
    }
   ],
   "source": [
    "# Precompute normalized overrides\n",
    "OVERRIDES = {\n",
    "    \"nationaltheatret\": \"Oslo\", \"kristiania\": \"Oslo\", \"christiania\": \"Oslo\",\n",
    "    \"det norske teatret\": \"Oslo\", \"black box teater\": \"Oslo\", \"oslo nye\": \"Oslo\",\n",
    "    \"trøndelag teater\": \"Trondheim\", \"rosendal teater\": \"Trondheim\",\n",
    "    \"den nationale scene\": \"Bergen\", \"dns\": \"Bergen\", \"hordaland teater\": \"Bergen\",\n",
    "    \"kilden\": \"Kristiansand\", \"agder teater\": \"Kristiansand\",\n",
    "    \"hålogaland teater\": \"Tromsø\", \"rogaland teater\": \"Stavanger\",\n",
    "    \"teater innsikt\": \"Stavanger\", \"teater i drammen\": \"Drammen\",\n",
    "    \"teater i fredrikstad\": \"Fredrikstad\", \"teater i moss\": \"Moss\",\n",
    "    \"teater i ålesund\": \"Ålesund\", \"teater i bodø\": \"Bodø\",\n",
    "    \"teater i tromsø\": \"Tromsø\", \"teater i sarpsborg\": \"Sarpsborg\",\n",
    "    \"teater i skien\": \"Skien\", \"teater i hamar\": \"Hamar\",\n",
    "    \"teater i sandnes\": \"Sandnes\"\n",
    "}\n",
    "\n",
    "# Precompute normalized overrides for faster lookup\n",
    "def normalize(txt):\n",
    "    txt = unicodedata.normalize('NFKD', txt)\n",
    "    txt = \"\".join(c for c in txt if not unicodedata.combining(c))\n",
    "    return re.sub(r'[^a-z0-9]', '', txt.lower())\n",
    "\n",
    "# Create normalized override mapping\n",
    "NORMALIZED_OVERRIDES = {normalize(k): v for k, v in OVERRIDES.items()}\n",
    "NORMALIZED_OVERRIDE_KEYS = list(NORMALIZED_OVERRIDES.keys())\n",
    "\n",
    "# Constants\n",
    "CITY_KEYS = [\n",
    "    \"city\", \"town\", \"village\", \"municipality\", \"hamlet\",\n",
    "    \"locality\", \"county\", \"state_district\", \"state\",\n",
    "    \"region\", \"district\", \"suburb\"\n",
    "]\n",
    "\n",
    "# Optimized city extraction logic\n",
    "def get_city_for(venue_name: str) -> str | None:\n",
    "    if not venue_name or pd.isna(venue_name):\n",
    "        return None\n",
    "    name = venue_name.strip()\n",
    "    \n",
    "    # 1. Check cache first (done for optimization)\n",
    "    if name in cache:\n",
    "        return cache[name]\n",
    "    \n",
    "    key = name.lower()\n",
    "    norm = normalize(key)\n",
    "\n",
    "    # 2. Hard overrides (using precomputed normalized keys)\n",
    "    for norm_key, city in NORMALIZED_OVERRIDES.items():\n",
    "        if norm_key in norm:\n",
    "            cache[name] = city\n",
    "            return city\n",
    "\n",
    "    # 3. Fuzzy match (only if no direct match found)\n",
    "    if len(norm) > 3:  # Skip very short names for fuzzy matching\n",
    "        match, score, _ = process.extractOne(norm, NORMALIZED_OVERRIDE_KEYS, scorer=fuzz.partial_ratio)\n",
    "        if score > 85:  # Slightly lower threshold for better performance\n",
    "            matched_city = NORMALIZED_OVERRIDES[match]\n",
    "            cache[name] = matched_city\n",
    "            return matched_city\n",
    "\n",
    "    # 4. Token match against known cities (before expensive API calls)\n",
    "    for token in re.split(r'[,/()\\-\\s]+', key):\n",
    "        token_cap = token.capitalize()\n",
    "        if token_cap in norway_cities:\n",
    "            cache[name] = token_cap\n",
    "            return token_cap\n",
    "\n",
    "    # 5. Geopy lookup (only for promising candidates)\n",
    "    if len(name) > 2 and any(char.isalpha() for char in name):\n",
    "        try:\n",
    "            loc = geocode(f\"{name}, Norway\", addressdetails=True, country_codes=\"no\")\n",
    "            if loc and (addr := loc.raw.get(\"address\")):\n",
    "                for k in CITY_KEYS:\n",
    "                    if k in addr:\n",
    "                        cache[name] = addr[k]\n",
    "                        return addr[k]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 6. GeoNames fallback (only for very specific cases)\n",
    "    if len(name) > 3 and name.count(' ') <= 2:  # Skip complex names\n",
    "        try:\n",
    "            qs = urllib.parse.urlencode({\n",
    "                \"q\": name,\n",
    "                \"country\": GEONAMES_COUNTRY,\n",
    "                \"maxRows\": 1,\n",
    "                \"username\": GEONAMES_USER\n",
    "            })\n",
    "            url = f\"http://api.geonames.org/searchJSON?{qs}\"\n",
    "            with urllib.request.urlopen(url, timeout=3) as resp:  # Reduced timeout\n",
    "                data = json.load(resp)\n",
    "                if data.get(\"geonames\"):\n",
    "                    city = data[\"geonames\"][0][\"name\"]\n",
    "                    cache[name] = city\n",
    "                    return city\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 7. Record failure\n",
    "    cache[name] = None\n",
    "    return None\n",
    "\n",
    "# Load and process input\n",
    "print(\"Loading input data...\")\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# Get unique venue names and filter out already cached ones\n",
    "unique_names = sorted({rec.get(\"venuename\") for rec in records if rec.get(\"venuename\")})\n",
    "uncached_names = [name for name in unique_names if name not in cache]\n",
    "\n",
    "print(f\"Total unique venues: {len(unique_names)}\")\n",
    "print(f\"Already cached: {len(unique_names) - len(uncached_names)}\")\n",
    "print(f\"Need to process: {len(uncached_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e5a33cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   5/   5] Vigelandsstua ved Lindesnes un → None (0.0/s)0/s)\n",
      " Processing complete in 0.0 seconds\n",
      "Annotating records and saving...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "venuecity filled for 3245 of 4924 records (65.90%)\n"
     ]
    }
   ],
   "source": [
    "# Process only uncached names\n",
    "start_time = time.time()\n",
    "for idx, vn in enumerate(uncached_names, start=1):\n",
    "    city = get_city_for(vn)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = idx / elapsed if elapsed > 0 else 0\n",
    "    print(f\"\\r[{idx:4d}/{len(uncached_names):4d}] {vn[:30]:30s} → {city} ({rate:.1f}/s)\", end=\"\")\n",
    "    \n",
    "    # Save cache more frequently for long-running processes\n",
    "    if idx % 50 == 0:\n",
    "        with open(CACHE_PATH, \"wb\") as cf:\n",
    "            pickle.dump(cache, cf)\n",
    "\n",
    "print(f\"\\n Processing complete in {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "# Annotate and save\n",
    "print(\"Annotating records and saving...\")\n",
    "for rec in records:\n",
    "    vn = rec.get(\"venuename\") or \"\"\n",
    "    rec[\"venuecity\"] = get_city_for(vn) or \"\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "with open(CACHE_PATH, \"wb\") as cf:\n",
    "    pickle.dump(cache, cf)\n",
    "\n",
    "# Stats\n",
    "filled = sum(1 for r in records if r.get(\"venuecity\") and r[\"venuecity\"].strip())\n",
    "total_records = len(records)\n",
    "print(f\"venuecity filled for {filled} of {total_records} records ({(filled / total_records) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bbd3a",
   "metadata": {},
   "source": [
    "The script above pre-processes only those unique venue names for geocoding, and later all 4900+ records are annotated using the chached lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960eaf20",
   "metadata": {},
   "source": [
    "### Mapping Works and Venues to authoritative IDs\n",
    "\n",
    "The next step is to map every key `workid` and `venueid` with an authoritative URI, to resolve external Wikidata URIs for works/venue. To achieve this result, the optimal way is to keep the original keys for `workid` and `venueid` from IbsenStage and add the keys `workURI` and `venueURI` to the json. First I will map the works and then the venues.\n",
    "\n",
    "A SPARQL query is sent to the Wikidata endpoint to retrieve all known works (`wdt:P800`) attributed to Henrik Ibsen (`wd:Q36661`). Labels are filtered to include multiple languages (en, no, nb, nn) and normalized to lowercase for matching. The script then attempts to match each worktitle from the dataset to a Wikidata label in two passes:\n",
    "\n",
    "1. Exact match based on normalized title.\n",
    "\n",
    "2. Partial string match (i.e. “Gjengangere” may match “Ghosts” or “The Ghosts”).\n",
    "\n",
    "If a match is found, a new field workURI is added with the corresponding Wikidata QID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3ce556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Ibsen works from Wikidata...\n",
      "First 5 results from Wikidata:\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q176465'}, 'label': {'xml:lang': 'en', 'type': 'literal', 'value': 'Hedda Gabler'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q176465'}, 'label': {'xml:lang': 'nb', 'type': 'literal', 'value': 'Hedda Gabler'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q176465'}, 'label': {'xml:lang': 'nn', 'type': 'literal', 'value': 'Hedda Gabler'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q208094'}, 'label': {'xml:lang': 'en', 'type': 'literal', 'value': 'Peer Gynt'}}\n",
      "{'work': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q208094'}, 'label': {'xml:lang': 'nb', 'type': 'literal', 'value': 'Peer Gynt'}}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('IbsenStage_with_city.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Query Wikidata for Henrik Ibsen's notable works, direct SPARQL query\n",
    "def get_ibsen_works():\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT ?work ?label WHERE {\n",
    "      wd:Q36661 wdt:P800 ?work .\n",
    "      ?work rdfs:label ?label .\n",
    "      FILTER(LANG(?label) IN (\"en\", \"no\", \"nb\", \"nn\"))\n",
    "    }\n",
    "    \"\"\"\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {'User-Agent': 'IbsenStage-Pipeline/1.0'}\n",
    "    params = {'query': sparql_query, 'format': 'json'}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"SPARQL query failed with status code {response.status_code}\")\n",
    "\n",
    "print(\"Fetching Ibsen works from Wikidata...\")\n",
    "wikidata_result = get_ibsen_works()\n",
    "\n",
    "# Preview first 5 entries\n",
    "print(\"First 5 results from Wikidata:\")\n",
    "for item in wikidata_result['results']['bindings'][:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd5c8456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54 labels from Wikidata\n",
      "First 5 label → QID mappings:\n",
      "hedda gabler → Q176465\n",
      "peer gynt → Q208094\n",
      "emperor and galilean → Q268276\n",
      "little eyolf → Q983970\n",
      "lille eyolf → Q983970\n"
     ]
    }
   ],
   "source": [
    "# Build title → QID mapping from SPARQL result\n",
    "wikidata_mapping = {}\n",
    "for item in wikidata_result['results']['bindings']:\n",
    "    uri = item['work']['value']\n",
    "    qid = uri.split('/')[-1]  # Extract QID from URI\n",
    "    label = item['label']['value'].lower().strip()\n",
    "    wikidata_mapping[label] = qid\n",
    "\n",
    "print(f\"Found {len(wikidata_mapping)} labels from Wikidata\")\n",
    "\n",
    "# Show first 5 items in the mapping\n",
    "print(\"First 5 label → QID mappings:\")\n",
    "for label, qid in list(wikidata_mapping.items())[:5]:\n",
    "    print(f\"{label} → {qid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf4f3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping 'worktitle' to Wikidata QIDs...\n",
      "Saved updated file to: IbsenStage_with_wikidata_works.json\n",
      "Successfully mapped 4905 out of 4924 work titles (99.6%)\n"
     ]
    }
   ],
   "source": [
    "# Function to map title to QID\n",
    "def map_to_qid(title):\n",
    "    if pd.isna(title) or not title.strip():\n",
    "        return None\n",
    "    title_norm = title.lower().strip()\n",
    "    \n",
    "    # Direct match\n",
    "    if title_norm in wikidata_mapping:\n",
    "        return wikidata_mapping[title_norm]\n",
    "    \n",
    "    # Partial match (fuzzy matching)\n",
    "    for label, qid in wikidata_mapping.items():\n",
    "        if title_norm in label or label in title_norm:\n",
    "            return qid\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Apply mapping to dataset\n",
    "print(\"Mapping 'worktitle' to Wikidata QIDs...\")\n",
    "df['workURI'] = df['worktitle'].apply(map_to_qid)\n",
    "\n",
    "# Save to file\n",
    "output_path = 'IbsenStage_with_wikidata_works.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved updated file to: {output_path}\")\n",
    "\n",
    "# Statistics\n",
    "mapped_count = df['workURI'].notna().sum()\n",
    "total_count = len(df)\n",
    "print(f\"Successfully mapped {mapped_count} out of {total_count} work titles ({mapped_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878ffcc",
   "metadata": {},
   "source": [
    "Now I can proceed by connecting `venueURI` to authoritative identifiers from Wikidata, where available.\n",
    "\n",
    "As expected, retrieving and mapping Wikidata URIs for all venues is a challenging process. Many venues are too small or obscure to have their own Wikidata item, or they may be part of a larger building. Some may no longer exist, as the IbsenStage dataset includes historical venues dating back to the 19th century.\n",
    "To address this, I use a two-step fallback strategy:\n",
    "\n",
    "1. Attempt to resolve the venuename to a `venueURI` by querying the Wikidata Search API (wbsearchentities).\n",
    "\n",
    "2. If no match is found, attempt to resolve the `venuecity` instead and store the resulting URI under the key `cityURI`.\n",
    "\n",
    "This method ensures that even if a venue does not have its own Wikidata item, the city associated with it can serve as a proxy reference.\n",
    "To improve performance, the code uses a thread pool (`ThreadPoolExecutor`); `aiohttp` for asynchronous HTTP requests and `asyncio.gather()` to run all tasks concurrently (with a semaphore to limit concurrency to `MAX_WORKERS`). Despite this optimization, the cell may still take several minutes to complete (around 7) due to the delay between requests and the need to wait for responses from the Wikidata servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef02610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION AND DATA LOADING COMPLETE\n",
      "Input file: IbsenStage_with_wikidata_works.json\n",
      "Output file: IbsenStage_with_uris.json\n",
      "User agent: VenueCityWikidataLinker/1.0\n",
      "Max concurrent workers: 5\n",
      "Delay between requests: 0.1s\n",
      "Total records loaded: 4924\n",
      "\n",
      "Sample records:\n",
      "  Record 1: Venue='Honningsvåg kino', City=''\n",
      "  Record 2: Venue='Vadsø kino', City='Vadsø'\n",
      "  Record 3: Venue='Miljøbygget', City='Trondheim'\n"
     ]
    }
   ],
   "source": [
    "INPUT = \"IbsenStage_with_wikidata_works.json\"\n",
    "OUTPUT = \"IbsenStage_with_uris.json\"\n",
    "USER_AGENT = \"VenueCityWikidataLinker/1.0\"\n",
    "MAX_WORKERS = 5\n",
    "DELAY = 0.1\n",
    "\n",
    "# logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# file path\n",
    "def stage_path(filename):\n",
    "    return Path.cwd() / filename\n",
    "\n",
    "# loading data\n",
    "with open(INPUT, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print samples\n",
    "print(\"CONFIGURATION AND DATA LOADING COMPLETE\")\n",
    "print(f\"Input file: {INPUT}\")\n",
    "print(f\"Output file: {OUTPUT}\")\n",
    "print(f\"User agent: {USER_AGENT}\")\n",
    "print(f\"Max concurrent workers: {MAX_WORKERS}\")\n",
    "print(f\"Delay between requests: {DELAY}s\")\n",
    "print(f\"Total records loaded: {len(data)}\")\n",
    "print(\"\\nSample records:\")\n",
    "for i, record in enumerate(data[:3]):\n",
    "    venue = record.get(\"venuename\", \"N/A\")\n",
    "    city = record.get(\"venuecity\", \"N/A\")\n",
    "    print(f\"  Record {i+1}: Venue='{venue}', City='{city}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8a618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STARTING WIKIDATA URI RESOLUTION\n",
      "Processing completed in 347.20 seconds\n",
      "\n",
      "=== PROCESSING RESULTS ===\n",
      "Venue URIs found: 2883\n",
      "City URIs found: 743\n",
      "Total URIs resolved: 3626 out of 4924 records (73.6%)\n",
      "\n",
      "Sample results with URIs:\n",
      "  Sample 1: Venue='Vadsø kino' (URI: N/A), City='Vadsø' (URI: Q104379)\n",
      "  Sample 2: Venue='Miljøbygget' (URI: N/A), City='Trondheim' (URI: Q25804)\n",
      "  Sample 3: Venue='Telemark Teater' (URI: Q4453420), City='' (URI: N/A)\n",
      "\n",
      "Mapping complete. Output saved to: IbsenStage_with_uris.json\n"
     ]
    }
   ],
   "source": [
    "# wikidata lookup helpers\n",
    "async def query_wikidata(session, search_term, semaphore):\n",
    "    if not search_term:\n",
    "        return None\n",
    "    async with semaphore:\n",
    "        await asyncio.sleep(DELAY)\n",
    "        params = urllib.parse.urlencode({\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"format\": \"json\",\n",
    "            \"language\": \"en\",\n",
    "            \"search\": search_term,\n",
    "            \"limit\": 1,\n",
    "            \"type\": \"item\"\n",
    "        })\n",
    "        url = f\"https://www.wikidata.org/w/api.php?{params}\"\n",
    "        headers = {\"User-Agent\": USER_AGENT}\n",
    "        try:\n",
    "            async with session.get(url, headers=headers, timeout=10) as response:\n",
    "                result = await response.json()\n",
    "                if result.get(\"search\"):\n",
    "                    return result[\"search\"][0][\"id\"]\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Wikidata query failed for '{search_term}': {e}\")\n",
    "        return None\n",
    "\n",
    "async def resolve_entry(session, entry, semaphore):\n",
    "    venue = (entry.get(\"venuename\") or \"\").strip()\n",
    "    city = (entry.get(\"venuecity\") or \"\").strip()\n",
    "    venue_uri = await query_wikidata(session, venue, semaphore)\n",
    "    if venue_uri:\n",
    "        entry[\"venueURI\"] = venue_uri\n",
    "    else:\n",
    "        city_uri = await query_wikidata(session, city, semaphore)\n",
    "        if city_uri:\n",
    "            entry[\"cityURI\"] = city_uri\n",
    "    return entry\n",
    "\n",
    "async def resolve_all(entries):\n",
    "    semaphore = asyncio.Semaphore(MAX_WORKERS)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [resolve_entry(session, entry, semaphore) for entry in entries]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "# resolution with await\n",
    "print(\"\\nSTARTING WIKIDATA URI RESOLUTION\")\n",
    "start_time = time.time()\n",
    "results = await resolve_all(data)\n",
    "processing_time = time.time() - start_time\n",
    "print(f\"Processing completed in {processing_time:.2f} seconds\")\n",
    "\n",
    "# stats\n",
    "venue_uris = sum(1 for r in results if r.get(\"venueURI\"))\n",
    "city_uris = sum(1 for r in results if r.get(\"cityURI\"))\n",
    "total_uris = venue_uris + city_uris\n",
    "\n",
    "print(\"\\nPROCESSING RESULTS\")\n",
    "print(f\"Venue URIs found: {venue_uris}\")\n",
    "print(f\"City URIs found: {city_uris}\")\n",
    "print(f\"Total URIs resolved: {total_uris} out of {len(results)} records ({(total_uris/len(results)*100):.1f}%)\")\n",
    "\n",
    "print(\"\\nSample results with URIs:\")\n",
    "uri_samples = [r for r in results if r.get(\"venueURI\") or r.get(\"cityURI\")][:3]\n",
    "for i, record in enumerate(uri_samples):\n",
    "    venue = record.get(\"venuename\", \"N/A\")\n",
    "    city = record.get(\"venuecity\", \"N/A\")\n",
    "    venue_uri = record.get(\"venueURI\", \"N/A\")\n",
    "    city_uri = record.get(\"cityURI\", \"N/A\")\n",
    "    print(f\"  Sample {i+1}: Venue='{venue}' (URI: {venue_uri}), City='{city}' (URI: {city_uri})\")\n",
    "\n",
    "# saving output\n",
    "with open(stage_path(OUTPUT), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nMapping complete. Output saved to: {OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d6753",
   "metadata": {},
   "source": [
    "Now that both work and venue IDs and URIs are present, we can bridge them in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ID-to-URI bridge saved to c:\\Users\\Cristiano (CC)\\Desktop\\Cristiano-June25\\OsloMet\\Masterstudium i bibliotek- og informasjonsvitenskap - deltid\\MBIB4140 - Metadata og interoperabilitet\\2ndre sjansen\\Ibsenstage_staged\\id_to_uri_bridge.json\n"
     ]
    }
   ],
   "source": [
    "# Load the enriched data\n",
    "with open('Ibsenstage_staged/IbsenStage_with_uris.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create bridge mappings\n",
    "work_bridge = (\n",
    "    df[['workid', 'workURI']]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .set_index('workid')['workURI']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "venue_bridge = (\n",
    "    df[['venueid', 'venueURI']]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .set_index('venueid')['venueURI']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Combine into one object\n",
    "bridge = {\n",
    "    'work_bridge': work_bridge,\n",
    "    'venue_bridge': venue_bridge\n",
    "}\n",
    "\n",
    "# Ensure staged directory exists\n",
    "staged_dir = Path.cwd() / 'Ibsenstage_staged'\n",
    "staged_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the mapping\n",
    "output_path = staged_dir / 'id_to_uri_bridge.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(bridge, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ID-to-URI bridge saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
