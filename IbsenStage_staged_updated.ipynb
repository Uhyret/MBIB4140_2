{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5f71a0",
   "metadata": {},
   "source": [
    "## Data Normalization and Metadata Preparation\n",
    "In this section, I normalize event titles using string operations. The raw JSON data, located in the folder `Ibsenstage_raw`, is flattened using `pandas.json_normalize()` for easier manipulation.\n",
    "I remove the `venuecountry` column, since the dataset is focused exclusively on performances in Norway. To standardize the naming of events, I build a canonical list of titles using the `worktitle` field, then construct regular expressions to match common title variants. Some well-known plays (i.e. `Et dukkehjem`) have additional hardcoded variant patterns (i.e. \"Nora\", \"Casa di bambola\", etc.).\n",
    "The `eventname` field is then normalized by matching against these compiled regex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf6ca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using source file: c:\\Users\\Cristiano (CC)\\Desktop\\Cristiano-June25\\OsloMet\\Masterstudium i bibliotek- og informasjonsvitenskap - deltid\\MBIB4140 - Metadata og interoperabilitet\\2ndre sjansen\\Ibsenstage_raw\\IbsenStage_scrape.json\n",
      "Saved normalized JSON → c:\\Users\\Cristiano (CC)\\Desktop\\Cristiano-June25\\OsloMet\\Masterstudium i bibliotek- og informasjonsvitenskap - deltid\\MBIB4140 - Metadata og interoperabilitet\\2ndre sjansen\\Ibsenstage_staged\\IbsenStage_normalized.json\n",
      "Preview:\n",
      "[\n",
      "  {\n",
      "    \"eventname\": \"Hedda Gabler\",\n",
      "    \"eventid\": 85542,\n",
      "    \"first_date\": \"1983-11-12\",\n",
      "    \"workid\": 8547.0,\n",
      "    \"worktitle\": \"Hedda Gabler\",\n",
      "    \"venueid\": 14985,\n",
      "    \"venuename\": \"Honningsvåg kino\"\n",
      "  },\n",
      "  {\n",
      "    \"eventname\": \"Hedda Gabler\",\n",
      "    \"eventid\": 85543,\n",
      "    \"first_date\": \"1983-11-14\",\n",
      "    \"workid\": 8547.0,\n",
      "    \"worktitle\": \"Hedda Gabler\",\n",
      "    \"venueid\": 14981,\n",
      "    \"venuename\": \"Vadsø kino\"\n",
      "  },\n",
      "  {\n",
      "    \"eventname\": \"Hedda Gabler\",\n",
      "    \"eventid\": 85544,\n",
      "    \"first_date\": \"1983-11-15\",\n",
      "    \"workid\": 8547.0,\n",
      "    \"worktitle\": \"Hedda Gabler\",\n",
      "    \"venueid\": 14983,\n",
      "    \"venuename\": \"Miljøbygget\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create and set the default output directory\n",
    "STAGED_DIR = Path.cwd() / \"Ibsenstage_staged\"\n",
    "STAGED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Patch open() and Path functions to redirect outputs to STAGED_DIR\n",
    "def stage_path(filename):\n",
    "    return STAGED_DIR / filename\n",
    "\n",
    "# Optimized normalization of event titles using vectorized pandas methods\n",
    "import json, re, shutil, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Locate the input file (within Ibsenstage_raw folder)\n",
    "raw_dir = Path.cwd() / 'Ibsenstage_raw'\n",
    "matches = list(raw_dir.rglob('IbsenStage_scrape.json'))\n",
    "if not matches:\n",
    "    raise FileNotFoundError('IbsenStage_scrape.json not found in Ibsenstage_raw folder.')\n",
    "src = matches[0]\n",
    "print('Using source file:', src)\n",
    "\n",
    "# 2) Ensure output folder exists\n",
    "staged_dir = Path.cwd() / 'Ibsenstage_staged'\n",
    "staged_dir.mkdir(exist_ok=True)\n",
    "json_out = staged_dir / 'IbsenStage_normalized.json'\n",
    "\n",
    "# 3) Load & flatten JSON\n",
    "with open(src, 'r', encoding='utf-8') as f:\n",
    "    root = json.load(f)\n",
    "records = root.get('hits', root)\n",
    "ibsen_df = pd.json_normalize(records, sep='_')\n",
    "\n",
    "# removing 'venuecountry'\n",
    "if 'venuecountry' in ibsen_df.columns:\n",
    "    ibsen_df = ibsen_df.drop(columns=['venuecountry'])\n",
    "\n",
    "# 4) Canonical regex patterns for work titles\n",
    "unique_titles = (\n",
    "    ibsen_df['worktitle']\n",
    "    .dropna()\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .sort_values()\n",
    "    .unique()\n",
    ")\n",
    "canonical = {}\n",
    "for title in unique_titles:\n",
    "    safe = re.escape(title).replace('\\\\\\\\ ', '[\\\\\\\\s_-]*')\n",
    "    canonical[title] = ['^' + safe + '$']\n",
    "extra_variants = {\n",
    "    'Et dukkehjem'   : ['^a doll.*house$', '^ett[\\\\\\\\s_-]*dockhem$', '^casa[\\\\\\\\s_-]*di[\\\\\\\\s_-]*bambola$', '^nora$'],\n",
    "    'Gjengangere'    : ['^ghosts$', '^spettri$'],\n",
    "    'En folkefiende' : ['^an enemy.*people$'],\n",
    "    'Vildanden'      : ['^the[\\\\\\\\s_-]*wild[\\\\\\\\s_-]*duck$'],\n",
    "}\n",
    "for canon, pats in extra_variants.items():\n",
    "    canonical.setdefault(canon, []).extend(pats)\n",
    "\n",
    "# 5) Compile all patterns and build matcher\n",
    "pattern_map = [\n",
    "    (re.compile(pat, re.IGNORECASE), canon)\n",
    "    for canon, pats in canonical.items()\n",
    "    for pat in pats\n",
    "]\n",
    "\n",
    "def normalize_title(txt):\n",
    "    if pd.isna(txt): return txt\n",
    "    low = str(txt).strip().lower()\n",
    "    for pat, canon in pattern_map:\n",
    "        if pat.match(low): return canon\n",
    "    return txt\n",
    "\n",
    "# 6) Normalize eventname IN PLACE\n",
    "ibsen_df['eventname'] = ibsen_df['eventname'].apply(normalize_title)\n",
    "\n",
    "# 7) Save updated JSON to staged folder\n",
    "ibsen_df.to_json(json_out, orient='records', force_ascii=False, indent=2)\n",
    "print('Saved normalized JSON →', json_out)\n",
    "\n",
    "# 8) Preview\n",
    "preview = json.loads(ibsen_df.head(3).to_json(orient='records', force_ascii=False))\n",
    "print('Preview:')\n",
    "print(json.dumps(preview, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0655166",
   "metadata": {},
   "source": [
    "Here I install Geopy - it will be useful in the coming cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85741185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: geopy==2.4.1 in c:\\users\\cristiano (cc)\\appdata\\roaming\\python\\python312\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\cristiano (cc)\\appdata\\roaming\\python\\python312\\site-packages (from geopy==2.4.1) (2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install geopy==2.4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94ef1e",
   "metadata": {},
   "source": [
    "### Loading and Preparing the Dataset\n",
    "The dataset is loaded and prepared with necessary libraries. This step ensures that I can apply transformations in a controlled and repeatable environment. I follow FAIR principles, especially focusing on reusability and interoperability. With this cell I give information about the cities connected to the venues, populating over 60% of the keys venuecity with actual city names thanks to GeoPy and GeoNames. \n",
    "To increase the accuracy, if the name of the city or a variation of it is included in the key `venuenames`, it will be mapped in the new key `venuecity` (i.e. \"Teater i Trondheim\" will give \"Trondheim\"). \n",
    "\n",
    "Here I also try to resolve some incongruencies, expecially related to Oslo, so that the key `venuecity` will connect back to that city in the instances of variation of Kristiania or Nationaltheatret. I also include some more common overrides to enhance the population. Ensuring the presence of `venuecity` is meaningful for a fallback when I will map `venueid` to URIs.\n",
    "\n",
    "I initialize a persistent cache to store previously resolved venue-to-city mappings (`venue_geocode_cache.pkl`). A separate city list from the GeoNames API (limited to Norway) is also cached to avoid repeated calls. Unicode normalization is applied to venue names to reduce inconsistencies due to accents or formatting, and the resolved cities are added to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b805407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n",
      "Total unique venues: 1302\n",
      "Already cached: 1297\n",
      "Need to process: 5\n",
      "[   5/   5] Vigelandsstua ved Lindesnes un → None (0.0/s)0/s)\n",
      "✔ Processing complete in 0.0 seconds\n",
      "Annotating records and saving...\n",
      "venuecity filled for 3245 of 4924 records (65.90%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import urllib.request, urllib.parse\n",
    "from urllib.error import HTTPError, URLError\n",
    "import unicodedata\n",
    "from rapidfuzz import process, fuzz\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "DATA_FOLDER     = \"Ibsenstage_staged\"\n",
    "CACHE_PATH      = os.path.join(DATA_FOLDER, \"venue_geocode_cache.pkl\")\n",
    "INPUT_FILE      = os.path.join(DATA_FOLDER, \"IbsenStage_normalized.json\")\n",
    "OUTPUT_FILE     = os.path.join(DATA_FOLDER, \"IbsenStage_with_city.json\")\n",
    "GEONAMES_USER   = \"MBIB4140_ibsen_user\"  # your GeoNames username\n",
    "GEONAMES_COUNTRY = \"NO\"\n",
    "\n",
    "# Nominatim Setup\n",
    "geolocator = Nominatim(user_agent=\"ibsen_city_extractor\", timeout=5)  # Reduced timeout\n",
    "geocode    = RateLimiter(geolocator.geocode, min_delay_seconds=0.5, max_retries=1)  # Faster rate limiting\n",
    "\n",
    "# Cache\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    with open(CACHE_PATH, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# Norway cities list (cached)\n",
    "def load_norway_cities():\n",
    "    cache_file = os.path.join(DATA_FOLDER, \"norway_cities_cache.pkl\")\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    qs = urllib.parse.urlencode({\n",
    "        \"country\": GEONAMES_COUNTRY,\n",
    "        \"featureClass\": \"P\",\n",
    "        \"maxRows\": 2000,\n",
    "        \"username\": GEONAMES_USER\n",
    "    })\n",
    "    url = f\"http://api.geonames.org/searchJSON?{qs}\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, timeout=10) as resp:\n",
    "            data = json.load(resp)\n",
    "            cities = {item[\"name\"] for item in data.get(\"geonames\", [])}\n",
    "            # Cache the cities list\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(cities, f)\n",
    "            return cities\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "norway_cities = load_norway_cities()\n",
    "\n",
    "# Precompute normalized overrides\n",
    "OVERRIDES = {\n",
    "    \"nationaltheatret\": \"Oslo\", \"kristiania\": \"Oslo\", \"christiania\": \"Oslo\",\n",
    "    \"det norske teatret\": \"Oslo\", \"black box teater\": \"Oslo\", \"oslo nye\": \"Oslo\",\n",
    "    \"trøndelag teater\": \"Trondheim\", \"rosendal teater\": \"Trondheim\",\n",
    "    \"den nationale scene\": \"Bergen\", \"dns\": \"Bergen\", \"hordaland teater\": \"Bergen\",\n",
    "    \"kilden\": \"Kristiansand\", \"agder teater\": \"Kristiansand\",\n",
    "    \"hålogaland teater\": \"Tromsø\", \"rogaland teater\": \"Stavanger\",\n",
    "    \"teater innsikt\": \"Stavanger\", \"teater i drammen\": \"Drammen\",\n",
    "    \"teater i fredrikstad\": \"Fredrikstad\", \"teater i moss\": \"Moss\",\n",
    "    \"teater i ålesund\": \"Ålesund\", \"teater i bodø\": \"Bodø\",\n",
    "    \"teater i tromsø\": \"Tromsø\", \"teater i sarpsborg\": \"Sarpsborg\",\n",
    "    \"teater i skien\": \"Skien\", \"teater i hamar\": \"Hamar\",\n",
    "    \"teater i sandnes\": \"Sandnes\"\n",
    "}\n",
    "\n",
    "# Precompute normalized overrides for faster lookup\n",
    "def normalize(txt):\n",
    "    txt = unicodedata.normalize('NFKD', txt)\n",
    "    txt = \"\".join(c for c in txt if not unicodedata.combining(c))\n",
    "    return re.sub(r'[^a-z0-9]', '', txt.lower())\n",
    "\n",
    "# Create normalized override mapping\n",
    "NORMALIZED_OVERRIDES = {normalize(k): v for k, v in OVERRIDES.items()}\n",
    "NORMALIZED_OVERRIDE_KEYS = list(NORMALIZED_OVERRIDES.keys())\n",
    "\n",
    "# Constants\n",
    "CITY_KEYS = [\n",
    "    \"city\", \"town\", \"village\", \"municipality\", \"hamlet\",\n",
    "    \"locality\", \"county\", \"state_district\", \"state\",\n",
    "    \"region\", \"district\", \"suburb\"\n",
    "]\n",
    "\n",
    "# Optimized city extraction logic\n",
    "def get_city_for(venue_name: str) -> str | None:\n",
    "    if not venue_name or pd.isna(venue_name):\n",
    "        return None\n",
    "    name = venue_name.strip()\n",
    "    \n",
    "    # 1. Check cache first (done for optimization)\n",
    "    if name in cache:\n",
    "        return cache[name]\n",
    "    \n",
    "    key = name.lower()\n",
    "    norm = normalize(key)\n",
    "\n",
    "    # 2. Hard overrides (using precomputed normalized keys)\n",
    "    for norm_key, city in NORMALIZED_OVERRIDES.items():\n",
    "        if norm_key in norm:\n",
    "            cache[name] = city\n",
    "            return city\n",
    "\n",
    "    # 3. Fuzzy match (only if no direct match found)\n",
    "    if len(norm) > 3:  # Skip very short names for fuzzy matching\n",
    "        match, score, _ = process.extractOne(norm, NORMALIZED_OVERRIDE_KEYS, scorer=fuzz.partial_ratio)\n",
    "        if score > 85:  # Slightly lower threshold for better performance\n",
    "            matched_city = NORMALIZED_OVERRIDES[match]\n",
    "            cache[name] = matched_city\n",
    "            return matched_city\n",
    "\n",
    "    # 4. Token match against known cities (before expensive API calls)\n",
    "    for token in re.split(r'[,/()\\-\\s]+', key):\n",
    "        token_cap = token.capitalize()\n",
    "        if token_cap in norway_cities:\n",
    "            cache[name] = token_cap\n",
    "            return token_cap\n",
    "\n",
    "    # 5. Geopy lookup (only for promising candidates)\n",
    "    if len(name) > 2 and any(char.isalpha() for char in name):\n",
    "        try:\n",
    "            loc = geocode(f\"{name}, Norway\", addressdetails=True, country_codes=\"no\")\n",
    "            if loc and (addr := loc.raw.get(\"address\")):\n",
    "                for k in CITY_KEYS:\n",
    "                    if k in addr:\n",
    "                        cache[name] = addr[k]\n",
    "                        return addr[k]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 6. GeoNames fallback (only for very specific cases)\n",
    "    if len(name) > 3 and name.count(' ') <= 2:  # Skip complex names\n",
    "        try:\n",
    "            qs = urllib.parse.urlencode({\n",
    "                \"q\": name,\n",
    "                \"country\": GEONAMES_COUNTRY,\n",
    "                \"maxRows\": 1,\n",
    "                \"username\": GEONAMES_USER\n",
    "            })\n",
    "            url = f\"http://api.geonames.org/searchJSON?{qs}\"\n",
    "            with urllib.request.urlopen(url, timeout=3) as resp:  # Reduced timeout\n",
    "                data = json.load(resp)\n",
    "                if data.get(\"geonames\"):\n",
    "                    city = data[\"geonames\"][0][\"name\"]\n",
    "                    cache[name] = city\n",
    "                    return city\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 7. Record failure\n",
    "    cache[name] = None\n",
    "    return None\n",
    "\n",
    "# Load and process input\n",
    "print(\"Loading input data...\")\n",
    "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# Get unique venue names and filter out already cached ones\n",
    "unique_names = sorted({rec.get(\"venuename\") for rec in records if rec.get(\"venuename\")})\n",
    "uncached_names = [name for name in unique_names if name not in cache]\n",
    "\n",
    "print(f\"Total unique venues: {len(unique_names)}\")\n",
    "print(f\"Already cached: {len(unique_names) - len(uncached_names)}\")\n",
    "print(f\"Need to process: {len(uncached_names)}\")\n",
    "\n",
    "# Process only uncached names\n",
    "start_time = time.time()\n",
    "for idx, vn in enumerate(uncached_names, start=1):\n",
    "    city = get_city_for(vn)\n",
    "    elapsed = time.time() - start_time\n",
    "    rate = idx / elapsed if elapsed > 0 else 0\n",
    "    print(f\"\\r[{idx:4d}/{len(uncached_names):4d}] {vn[:30]:30s} → {city} ({rate:.1f}/s)\", end=\"\")\n",
    "    \n",
    "    # Save cache more frequently for long-running processes\n",
    "    if idx % 50 == 0:\n",
    "        with open(CACHE_PATH, \"wb\") as cf:\n",
    "            pickle.dump(cache, cf)\n",
    "\n",
    "print(f\"\\n✔ Processing complete in {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "# Annotate and save\n",
    "print(\"Annotating records and saving...\")\n",
    "for rec in records:\n",
    "    vn = rec.get(\"venuename\") or \"\"\n",
    "    rec[\"venuecity\"] = get_city_for(vn) or \"\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "with open(CACHE_PATH, \"wb\") as cf:\n",
    "    pickle.dump(cache, cf)\n",
    "\n",
    "# Stats\n",
    "filled = sum(1 for r in records if r.get(\"venuecity\") and r[\"venuecity\"].strip())\n",
    "total_records = len(records)\n",
    "print(f\"venuecity filled for {filled} of {total_records} records ({(filled / total_records) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bbd3a",
   "metadata": {},
   "source": [
    "The script above pre-processes only those unique venue names for geocoding, and later all 4900+ records are annotated using the chached lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960eaf20",
   "metadata": {},
   "source": [
    "### Mapping Works and Venues to authoritative IDs\n",
    "\n",
    "The next step is to map every key `workid` and `venueid` with an authoritative URI, to resolve external Wikidata URIs for works/venue. To achieve this result, the optimal way is to keep the original keys for `workid` and `venueid` from IbsenStage and add the keys `workURI` and `venueURI` to the json. First I will map the works and then the venues.\n",
    "\n",
    "A SPARQL query is sent to the Wikidata endpoint to retrieve all known works (`wdt:P800`) attributed to Henrik Ibsen (`wd:Q36661`). Labels are filtered to include multiple languages (en, no, nb, nn) and normalized to lowercase for matching. The script then attempts to match each worktitle from the dataset to a Wikidata label in two passes:\n",
    "\n",
    "1. Exact match based on normalized title.\n",
    "\n",
    "2. Partial string match (i.e. “Gjengangere” may match “Ghosts” or “The Ghosts”).\n",
    "\n",
    "If a match is found, a new field workURI is added with the corresponding Wikidata URI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3ce556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Ibsen works from Wikidata...\n",
      "Found 54 labels from Wikidata\n",
      "Mapping 'worktitle' to Wikidata QIDs...\n",
      "Saved updated file to: Ibsenstage_staged/IbsenStage_with_wikidata_works.json\n",
      "Successfully mapped 4905 out of 4924 work titles (99.6%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load the dataset\n",
    "with open('Ibsenstage_staged/IbsenStage_with_city.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Query Wikidata for Henrik Ibsen's notable works\n",
    "def get_ibsen_works():\n",
    "    sparql_query = \"\"\"\n",
    "    SELECT ?work ?label WHERE {\n",
    "      wd:Q36661 wdt:P800 ?work .\n",
    "      ?work rdfs:label ?label .\n",
    "      FILTER(LANG(?label) IN (\"en\", \"no\", \"nb\", \"nn\"))\n",
    "    }\n",
    "    \"\"\"\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    headers = {'User-Agent': 'IbsenStage-Pipeline/1.0'}\n",
    "    params = {'query': sparql_query, 'format': 'json'}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"SPARQL query failed with status code {response.status_code}\")\n",
    "\n",
    "print(\"Fetching Ibsen works from Wikidata...\")\n",
    "wikidata_result = get_ibsen_works()\n",
    "#print (wikidata_result.head(5))\n",
    "\n",
    "# Build title → QID mapping from SPARQL result\n",
    "wikidata_mapping = {}\n",
    "for item in wikidata_result['results']['bindings']:\n",
    "    uri = item['work']['value']\n",
    "    qid = uri.split('/')[-1]  # Extract QID from URI (e.g., Q1432009)\n",
    "    label = item['label']['value'].lower().strip()\n",
    "    wikidata_mapping[label] = qid\n",
    "\n",
    "print(f\"Found {len(wikidata_mapping)} labels from Wikidata\")\n",
    "#wikidata_mapping.head(5)\n",
    "# Function to map title to QID\n",
    "def map_to_qid(title):\n",
    "    if pd.isna(title) or not title.strip():\n",
    "        return None\n",
    "    title_norm = title.lower().strip()\n",
    "    \n",
    "    # Direct match\n",
    "    if title_norm in wikidata_mapping:\n",
    "        return wikidata_mapping[title_norm]\n",
    "    \n",
    "    # Partial match (fuzzy matching)\n",
    "    for label, qid in wikidata_mapping.items():\n",
    "        if title_norm in label or label in title_norm:\n",
    "            return qid\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Apply mapping to dataset\n",
    "print(\"Mapping 'worktitle' to Wikidata QIDs...\")\n",
    "df['workURI'] = df['worktitle'].apply(map_to_qid)\n",
    "\n",
    "# Save to file\n",
    "output_path = 'Ibsenstage_staged/IbsenStage_with_wikidata_works.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved updated file to: {output_path}\")\n",
    "\n",
    "# Statistics\n",
    "mapped_count = df['workURI'].notna().sum()\n",
    "total_count = len(df)\n",
    "print(f\"Successfully mapped {mapped_count} out of {total_count} work titles ({mapped_count/total_count*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878ffcc",
   "metadata": {},
   "source": [
    "Now I can proceed by connecting `venueURI` with an authoritative ID for the venues when available in WikiData. \n",
    "\n",
    "As expected, fetching and mapping authoritative URIs for all the venues is a difficult process, since many of these venues are either too small to possess their own URI or part of a bigger building/venue. Some of these venues might not even exists nowadays, as the data scraped from IbsenStage includes venues from the 19th cenutury.\n",
    "\n",
    "Since not all venues can be tracked through WikiData, to partially resolve this problem I will try as a fallback to map the cities' URIs to the venues not found in WikiData. The cities URI will be present in a new key called `cityURI`. To address this, the code follows a two-step fallback strategy:\n",
    "\n",
    "1. It first attempts to resolve the venuename to a venueURI using the Wikidata Search API (`wbsearchentities`).\n",
    "\n",
    "2. If no match is found, it tries to resolve the associated venuecity instead, storing the result in a separate field, cityURI.\n",
    "\n",
    "The process uses a thread pool (`ThreadPoolExecutor`) to perform multiple lookups concurrently (up to 5 at a time), while throttling requests using a brief delay to avoid overloading Wikidata’s servers. Any failed or timed-out queries are caught and logged for debugging.\n",
    "\n",
    "(This cell might require up to 7 minutes to run). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef02610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping complete. Output saved to: IbsenStage_with_uris.json\n"
     ]
    }
   ],
   "source": [
    "import json, urllib.request, urllib.parse, time, logging, asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Config\n",
    "INPUT = \"IbsenStage_with_wikidata_works.json\"\n",
    "OUTPUT = \"IbsenStage_with_uris.json\"\n",
    "USER_AGENT = \"VenueCityWikidataLinker/1.0\"\n",
    "MAX_WORKERS = 5\n",
    "DELAY = 0.1\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "async def query_wikidata(search_term):\n",
    "    time.sleep(DELAY)\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"format\": \"json\",\n",
    "        \"language\": \"en\",\n",
    "        \"search\": search_term,\n",
    "        \"limit\": 1,\n",
    "        \"type\": \"item\"\n",
    "    })\n",
    "    url = f\"https://www.wikidata.org/w/api.php?{params}\"\n",
    "    try:\n",
    "        req = urllib.request.Request(url, headers={\"User-Agent\": USER_AGENT})\n",
    "        with urllib.request.urlopen(req, timeout=10) as res:\n",
    "            result = json.loads(res.read())\n",
    "            if result.get(\"search\"):\n",
    "                return result[\"search\"][0][\"id\"]\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Wikidata query failed for '{search_term}': {e}\")\n",
    "    return None\n",
    "\n",
    "async def resolve_uris(entry):\n",
    "    venue = (entry.get(\"venuename\") or \"\").strip()\n",
    "    city = (entry.get(\"venuecity\") or \"\").strip()\n",
    "    venue_uri = await query_wikidata(venue) if venue else None\n",
    "    if venue_uri:\n",
    "        entry[\"venueURI\"] = venue_uri\n",
    "    else:\n",
    "        city_uri = await query_wikidata(city) if city else None\n",
    "        if city_uri:\n",
    "            entry[\"cityURI\"] = city_uri\n",
    "    return entry\n",
    "\n",
    "# Load and process\n",
    "with open(INPUT, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def get_uris(entry):\n",
    "    return asyncio.run(resolve_uris(entry))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(get_uris, entry) for entry in data]\n",
    "    results = [future.result() for future in as_completed(futures)]\n",
    "\n",
    "with open(stage_path(OUTPUT), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Mapping complete. Output saved to:\", OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d6753",
   "metadata": {},
   "source": [
    "Now that both work and venue IDs and URIs are present, we can bridge them in a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457672a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ID-to-URI bridge saved to c:\\Users\\Cristiano (CC)\\Desktop\\Cristiano-June25\\OsloMet\\Masterstudium i bibliotek- og informasjonsvitenskap - deltid\\MBIB4140 - Metadata og interoperabilitet\\2ndre sjansen\\Ibsenstage_staged\\id_to_uri_bridge.json\n"
     ]
    }
   ],
   "source": [
    "# Load the enriched data\n",
    "with open('Ibsenstage_staged/IbsenStage_with_uris.json', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create bridge mappings\n",
    "work_bridge = (\n",
    "    df[['workid', 'workURI']]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .set_index('workid')['workURI']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "venue_bridge = (\n",
    "    df[['venueid', 'venueURI']]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .set_index('venueid')['venueURI']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Combine into one object\n",
    "bridge = {\n",
    "    'work_bridge': work_bridge,\n",
    "    'venue_bridge': venue_bridge\n",
    "}\n",
    "\n",
    "# Ensure staged directory exists\n",
    "staged_dir = Path.cwd() / 'Ibsenstage_staged'\n",
    "staged_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the mapping\n",
    "output_path = staged_dir / 'id_to_uri_bridge.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(bridge, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"ID-to-URI bridge saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027719bd",
   "metadata": {},
   "source": [
    "### Prerequisites Setup\n",
    "All required dependencies are set up here. This supports a production pipeline approach where reproducibility and environment setup are clearly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Prereqs ──────────────────────────────────────────────────────────────────\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import pandas as pd\n",
    "import pickle, os\n",
    "\n",
    "# 1) Geocoder + rate-limiting\n",
    "geolocator = Nominatim(user_agent=\"ibsen_city_extractor\", timeout=10)\n",
    "geocode    = RateLimiter(geolocator.geocode,   min_delay_seconds=1, max_retries=2)\n",
    "reverse    = RateLimiter(geolocator.reverse,   min_delay_seconds=1, max_retries=2)\n",
    "\n",
    "# 2) On-disk cache\n",
    "CACHE = \"venue_geocode_cache.pkl\"\n",
    "if os.path.exists(CACHE):\n",
    "    with open(CACHE, \"rb\") as f:\n",
    "        cache = pickle.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "# 3) City-level keys, in priority order\n",
    "CITY_KEYS = [\n",
    "    \"city\",\"town\",\"village\",\"municipality\",\n",
    "    \"hamlet\",\"locality\",\"county\",\"state_district\",\n",
    "    \"state\",\"region\",\"district\",\"suburb\"\n",
    "]\n",
    "\n",
    "# 4) Optional: load an exhaustive list of Norwegian municipalities\n",
    "#    (download from Kartverket or any public CSV).\n",
    "#    e.g. muni_df = pd.read_csv(\"norway_municipalities.csv\")[\"municipality\"].tolist()\n",
    "municipalities = set()  # fill this if you have a CSV\n",
    "\n",
    "# ─── New get_city_for ─────────────────────────────────────────────────────────\n",
    "def get_city_for(venue_name: str) -> str | None:\n",
    "    if pd.isna(venue_name) or not venue_name.strip():\n",
    "        return None\n",
    "    name = venue_name.strip()\n",
    "\n",
    "    # a) cache hit?\n",
    "    if name in cache:\n",
    "        return cache[name]\n",
    "\n",
    "    city = None\n",
    "\n",
    "    # b) forward geocode\n",
    "    try:\n",
    "        q = f\"{name}, Norway\"\n",
    "        loc = geocode(q, addressdetails=True, country_codes=\"no\")\n",
    "        if loc and \"address\" in loc.raw:\n",
    "            addr = loc.raw[\"address\"]\n",
    "            for key in CITY_KEYS:\n",
    "                if key in addr:\n",
    "                    city = addr[key]\n",
    "                    break\n",
    "\n",
    "        # c) reverse geocode fallback (if forward gave us coords but no city)\n",
    "        if city is None and loc:\n",
    "            rev = reverse((loc.latitude, loc.longitude),\n",
    "                          addressdetails=True, country_codes=\"no\")\n",
    "            if rev and \"address\" in rev.raw:\n",
    "                for key in CITY_KEYS:\n",
    "                    if key in rev.raw[\"address\"]:\n",
    "                        city = rev.raw[\"address\"][key]\n",
    "                        break\n",
    "\n",
    "        # d) display_name parsing: split on commas, look for known muni\n",
    "        if city is None and loc and loc.raw.get(\"display_name\"):\n",
    "            parts = [p.strip() for p in loc.raw[\"display_name\"].split(\",\")]\n",
    "            # check last few parts\n",
    "            for part in parts[-4:]:\n",
    "                if part in municipalities:\n",
    "                    city = part\n",
    "                    break\n",
    "\n",
    "    except Exception:\n",
    "        city = None\n",
    "\n",
    "    # e) record result (even if None) so I don’t retry\n",
    "    cache[name] = city\n",
    "    return city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adee035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
