{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fea046a",
   "metadata": {},
   "source": [
    "## Prerequisites pt.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0889a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, math, datetime\n",
    "from pathlib import Path\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, DCTERMS, XSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52f316",
   "metadata": {},
   "source": [
    "# RDF Creation\n",
    "\n",
    "In this document I will proceed in the pipeline by generating a file containing RDF triplets. I start from the previous file, \"IbsenStage_with_uris.json\". In this step i use `rdflib`. As explained in the main documentation, each theatre event becomes a `schema:TheaterEvent`, linked to a play (`schema:Play`) and a venue (`schema:EventVenue`).\n",
    "Where available, the script links works and venues to Wikidata URIs via `schema:sameAs`. It also adds normalized performance dates (`xsd:date`), event names, and internal IDs. If the venue’s URI isn’t available, the city’s Wikidata URI is used as a fallback (`schema:location`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc173ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4924 theater performance records\n"
     ]
    }
   ],
   "source": [
    "# I locate the \"Ibsenstage_staged\" folder containing the input data\n",
    "cwd = Path.cwd()\n",
    "root = cwd\n",
    "while not (root / \"Ibsenstage_staged\").exists() and root.parent != root:\n",
    "    root = root.parent\n",
    "\n",
    "# I set up file paths for reading JSON input and writing RDF output\n",
    "input_path = root / \"Ibsenstage_staged\" / \"IbsenStage_with_uris.json\"\n",
    "if not input_path.exists():\n",
    "    raise FileNotFoundError(f\"Cannot find file: {input_path}\")\n",
    "output_dir = root / \"Ibsenstage_curated\"\n",
    "if not output_dir.exists():\n",
    "    raise FileNotFoundError(f\"Cannot find directory: {output_dir}\")\n",
    "output_path = output_dir / \"ibsenstage_triplets.ttl\"\n",
    "\n",
    "# here I create RDF namespaces\n",
    "SCHEMA = Namespace(\"http://schema.org/\")          \n",
    "WD = Namespace(\"https://www.wikidata.org/entity/\")\n",
    "IBSEN = Namespace(\"https://ibsenstage.hf.uio.no/pages/\") # The custom namespace for Ibsen data\n",
    "\n",
    "# create an empty RDF graph and register our namespaces with it\n",
    "g = Graph()\n",
    "g.bind(\"schema\", SCHEMA)\n",
    "g.bind(\"dcterms\", DCTERMS)\n",
    "g.bind(\"wd\", WD)\n",
    "g.bind(\"xsd\", XSD)\n",
    "\n",
    "# function to clean up messy date formats and make them valid for RDF\n",
    "def normalize_date(date_str) -> tuple[str | None, type | None]:\n",
    "    if not date_str:\n",
    "        return None, None\n",
    "    s = str(date_str).strip()\n",
    "    \n",
    "    # check if it's already a complete date (YYYY-MM-DD)\n",
    "    m = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\", s)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        try:\n",
    "            datetime.date(y, mo, d)  # Verify it's a real date\n",
    "            return s, XSD.date\n",
    "        except ValueError:\n",
    "            return s, None\n",
    "    \n",
    "    # then I fill in missing parts: \"1879-03\" becomes \"1879-03-01\"\n",
    "    if re.match(r\"^\\d{4}-\\d{2}$\", s):\n",
    "        return s + \"-01\", XSD.date\n",
    "    \n",
    "    # here \"1879\" becomes \"1879-01-01\"\n",
    "    if re.match(r\"^\\d{4}$\", s):\n",
    "        return s + \"-01-01\", XSD.date\n",
    "\n",
    "    # if it doesn't match any known format, return as it is    \n",
    "    return s, None\n",
    "\n",
    "# Load the theater data from our JSON file\n",
    "data = json.loads(input_path.read_text(encoding=\"utf-8\"))\n",
    "print(f\"Loaded {len(data)} theater performance records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc10202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first 50 records to verify structure...\n",
      "Generated 469 RDF triples from 50 records\n",
      "\n",
      "Sample RDF triples:\n",
      "  https://ibsenstage.hf.uio.no/pages/venue/15014 http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://schema.org/EventVenue\n",
      "  https://ibsenstage.hf.uio.no/pages/event/85934 http://purl.org/dc/terms/identifier 85934\n",
      "  https://ibsenstage.hf.uio.no/pages/work/8528 http://purl.org/dc/terms/identifier 8528\n",
      "  https://ibsenstage.hf.uio.no/pages/event/85772 http://schema.org/name Een poppenhuis\n",
      "  https://ibsenstage.hf.uio.no/pages/event/85768 http://schema.org/workPerformed https://www.wikidata.org/entity/Q1434818\n",
      "  https://ibsenstage.hf.uio.no/pages/event/85612 http://schema.org/name Rosmersholm\n",
      "  https://ibsenstage.hf.uio.no/pages/event/85765 http://schema.org/name Gengangere\n",
      "  https://ibsenstage.hf.uio.no/pages/venue/12427 http://purl.org/dc/terms/identifier 12427\n",
      "  https://ibsenstage.hf.uio.no/pages/work/8544 http://schema.org/sameAs https://www.wikidata.org/entity/Q1217608\n",
      "  https://ibsenstage.hf.uio.no/pages/event/85769 http://schema.org/name Gengangere\n",
      "\n",
      "... and 459 more triples\n"
     ]
    }
   ],
   "source": [
    "# Function to process a single theater record and add RDF triples\n",
    "def process_record(rec, graph):\n",
    "    # extract the basic IDs that identify performances, plays, and venues\n",
    "    eid = str(rec.get(\"eventid\"))\n",
    "    wid = rec.get(\"workid\")\n",
    "    vid = str(rec.get(\"venueid\"))\n",
    "\n",
    "    # convert work ID to string, or None if missing/invalid\n",
    "    wid = str(int(wid)) if wid and not math.isnan(wid) else None\n",
    "\n",
    "    # look for Wikidata references that link my data to external knowledge\n",
    "    # these URIs connect local theater data to global knowledge bases\n",
    "    work_ref = None\n",
    "    raw_work = rec.get(\"workURI\")  # Wikidata URI for the play\n",
    "    if raw_work:\n",
    "        # extract just the QID part (i.e. \"Q1234\") from the full URI\n",
    "        qid = raw_work if raw_work.startswith(\"Q\") else raw_work.rsplit(\"/\", 1)[-1]\n",
    "        work_ref = WD[qid]\n",
    "\n",
    "    venue_ref = None\n",
    "    raw_venue = rec.get(\"venueURI\")  # Wikidata URI for the venue\n",
    "    if raw_venue:\n",
    "        qid_v = raw_venue if raw_venue.startswith(\"Q\") else raw_venue.rsplit(\"/\", 1)[-1]\n",
    "        venue_ref = WD[qid_v]\n",
    "\n",
    "    # create internal URIs for my data using the custom namespace\n",
    "    event_res = IBSEN[f\"event/{eid}\"]\n",
    "    work_res = IBSEN[f\"work/{wid}\"] if wid else None\n",
    "    venue_res = IBSEN[f\"venue/{vid}\"]\n",
    "\n",
    "    # declare events as a theater performance\n",
    "    graph.add((event_res, RDF.type, SCHEMA.TheaterEvent))\n",
    "    \n",
    "    # add the performance name/title\n",
    "    name = rec.get(\"eventname_normalized\") or rec.get(\"eventname\")\n",
    "    if name:\n",
    "        graph.add((event_res, SCHEMA.name, Literal(name)))\n",
    "\n",
    "    # add the date of first performance, using cleaned-up date format\n",
    "    first_date = rec.get(\"first_date\")\n",
    "    if first_date:\n",
    "        lex, dt = normalize_date(first_date)\n",
    "        if lex:\n",
    "            # create a properly typed date literal for RDF\n",
    "            lit = Literal(lex, datatype=dt) if dt else Literal(lex)\n",
    "            graph.add((event_res, SCHEMA.firstPerformance, lit))\n",
    "\n",
    "    # link the performances to the Wikidata entry for the play\n",
    "    if work_ref:\n",
    "        graph.add((event_res, SCHEMA.workPerformed, work_ref))\n",
    "\n",
    "    # add the city where this performance took place\n",
    "    raw_city = rec.get(\"cityURI\")\n",
    "    if raw_city:\n",
    "        qid_c = raw_city if raw_city.startswith(\"Q\") else raw_city.rsplit(\"/\", 1)[-1]\n",
    "        graph.add((event_res, SCHEMA.location, WD[qid_c]))\n",
    "\n",
    "    # add DCterms internal identifier for tracking\n",
    "    graph.add((event_res, DCTERMS.identifier, Literal(eid)))\n",
    "\n",
    "    # create a separate entry for plays and declare it as a schema:Play\n",
    "    if work_res:\n",
    "        graph.add((work_res, RDF.type, SCHEMA.Play))\n",
    "        title = rec.get(\"worktitle\")\n",
    "        if title:\n",
    "            graph.add((work_res, SCHEMA.name, Literal(title)))  # add play title\n",
    "        graph.add((work_res, DCTERMS.identifier, Literal(wid))) # add ID\n",
    "        if work_ref:\n",
    "            # link local play entries to Wikidata entries\n",
    "            graph.add((work_res, SCHEMA.sameAs, work_ref))\n",
    "\n",
    "    # create entries for the venue\n",
    "    if venue_ref:\n",
    "        # link local venue to its Wikidata entry\n",
    "        graph.add((venue_res, SCHEMA.sameAs, venue_ref))\n",
    "    if venue_res:\n",
    "        graph.add((venue_res, RDF.type, SCHEMA.EventVenue))  # declare it as a venue\n",
    "        vname = rec.get(\"venuename\", \"\")\n",
    "        if vname:\n",
    "            graph.add((venue_res, SCHEMA.name, Literal(vname)))  # add venue name\n",
    "        graph.add((venue_res, DCTERMS.identifier, Literal(vid))) # add ID\n",
    "\n",
    "# process a small batch to verify if RDF structure works correctly\n",
    "sample_size = 50\n",
    "print(f\"Processing first {sample_size} records to verify structure...\")\n",
    "\n",
    "# loop through each theater performance record in sample\n",
    "for i, rec in enumerate(data[:sample_size]):\n",
    "    process_record(rec, g)\n",
    "\n",
    "print(f\"Generated {len(g)} RDF triples from {sample_size} records\")\n",
    "\n",
    "# show a sample of what my RDF data looks like\n",
    "print(\"\\nSample RDF triples:\")\n",
    "for i, (s, p, o) in enumerate(g):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  {s} {p} {o}\")\n",
    "    \n",
    "print(f\"\\n... and {len(g) - 10} more triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983d4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing remaining 4874 records...\n",
      "Processed 1000/4924 records (7407 triples so far)\n",
      "Processed 2000/4924 records (13468 triples so far)\n",
      "Processed 3000/4924 records (20276 triples so far)\n",
      "Processed 4000/4924 records (26273 triples so far)\n",
      "Completed processing all 4924 records\n",
      "Saved 31739 RDF triples to C:\\Users\\Cristiano (CC)\\Desktop\\Cristiano-June25\\OsloMet\\Masterstudium i bibliotek- og informasjonsvitenskap - deltid\\MBIB4140 - Metadata og interoperabilitet\\2ndre sjansen\\MBIB4140_2\\Ibsenstage_curated\\ibsenstage_triplets.ttl\n",
      "\n",
      "Final RDF graph includes:\n",
      "  4924 theater events\n",
      "  30 different plays\n",
      "  1400 theater venues\n"
     ]
    }
   ],
   "source": [
    "# here I process the remaining records\n",
    "print(f\"Processing remaining {len(data) - 50} records...\")\n",
    "\n",
    "for i, rec in enumerate(data[50:], start=50):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Processed {i}/{len(data)} records ({len(g)} triples so far)\")\n",
    "    \n",
    "    # reuse the processing function\n",
    "    process_record(rec, g)\n",
    "\n",
    "print(f\"Completed processing all {len(data)} records\")\n",
    "\n",
    "# then convert RDF graph to Turtle format and save\n",
    "ttl_output = g.serialize(format=\"turtle\")\n",
    "ttl_output = \"@prefix wd: <https://www.wikidata.org/entity/> .\\n\" + ttl_output\n",
    "\n",
    "output_path.write_text(ttl_output, encoding=\"utf-8\")\n",
    "print(f\"Saved {len(g)} RDF triples to {output_path.resolve()}\")\n",
    "\n",
    "# check how many theater events, plays, and venues I have in the final graph\n",
    "events = sum(1 for s, p, o in g if p == RDF.type and o == SCHEMA.TheaterEvent)\n",
    "works = sum(1 for s, p, o in g if p == RDF.type and o == SCHEMA.Play)\n",
    "venues = sum(1 for s, p, o in g if p == RDF.type and o == SCHEMA.EventVenue)\n",
    "\n",
    "print(f\"\\nFinal RDF graph includes:\")\n",
    "print(f\"  {events} theater events\")\n",
    "print(f\"  {works} different plays\")  \n",
    "print(f\"  {venues} theater venues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348771dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "421c1017",
   "metadata": {},
   "source": [
    "# Serialization in RDF/XML\n",
    "\n",
    "The next step is serializing the triplets obtained also in RDF/XML. Serialization changes the RDF graph into a file format that semantic web tools can use to store, share, or read data. While the Turtle file obtained in the previous cell is concise and easy for humans to read and debug, RDF/XML is the original W3C-standard serialization for RDF and remains widely supported by legacy systems, ontology tools, and triplestores. It is more formal, and it integrates easily with XML-based workflows, allows validation using standard XML tools, and ensures maximum interoperability in institutional contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a4726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 30442 triples to:\n",
      "   - C:\\Users\\Cristiano (CC)\\Desktop\\Cristiano-June25\\OsloMet\\Masterstudium i bibliotek- og informasjonsvitenskap - deltid\\MBIB4140 - Metadata og interoperabilitet\\2ndre sjansen\\Ibsenstage_curated\\ibsenstage_triplets.rdf\n"
     ]
    }
   ],
   "source": [
    "# define output path\n",
    "rdfxml_path = output_dir / \"ibsenstage_triplets.rdf\"\n",
    "\n",
    "# serialize to RDF/XML\n",
    "rdfxml_output = g.serialize(format=\"xml\")\n",
    "rdfxml_path.write_text(rdfxml_output, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved {len(g)} triples to:\")\n",
    "print(f\" - {rdfxml_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c40d09",
   "metadata": {},
   "source": [
    "# Validating\n",
    "\n",
    "The final step is verifying that the RDF file is well-formed. To do so, I will first check if there are any parsing or syntaxt issues locally, then I will make the RDF accessible via an URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDF/XML is valid.\n"
     ]
    }
   ],
   "source": [
    "#finally I check if the RDF is well-formed\n",
    "g = Graph()\n",
    "try:\n",
    "    g.parse(\"Ibsenstage_curated/ibsenstage_triplets.rdf\", format=\"xml\")\n",
    "    print(\"RDF/XML is valid.\")\n",
    "except Exception as e:\n",
    "    print(\"RDF/XML is invalid:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4eabe8",
   "metadata": {},
   "source": [
    "The whole project has been uploaded in the repository `MBIB4140_2` on GitHub, and it is publicly available at the link: [https://github.com/Uhyret/MBIB4140_2] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
